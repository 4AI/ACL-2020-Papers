{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "generate_paper_list_with_arxiv_link.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1m2RpTeiCNQq"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hunkim/ACL-2020-Papers/blob/master/generate_paper_list_with_arxiv_link.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m2RpTeiCNQq",
        "colab_type": "text"
      },
      "source": [
        "# Load Paper List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G-XWU8JCNQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_papers(path):\n",
        "    papers = [[]]\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                papers[-1].append(line)\n",
        "            else:\n",
        "                papers.append([])\n",
        "    for p in papers:\n",
        "        assert len(p) == 2\n",
        "    return papers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_5mqAPzCNQu",
        "colab_type": "code",
        "outputId": "c0f334d7-f482-4fdd-85d9-c00a3d7620ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "longp = read_papers(\"./data/long.txt\")\n",
        "longp[:3]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['2kenize: Tying Subword Sequences for Chinese Script Conversion',\n",
              "  'Pranav A and Isabelle Augenstein'],\n",
              " ['A Batch Normalized Inference Network Keeps the KL Vanishing Away',\n",
              "  'Qile Zhu, Wei Bi, Xiaojiang Liu, Xiyao Ma, Xiaolin Li and Dapeng Wu'],\n",
              " ['A Call for More Rigor in Unsupervised Cross-lingual Learning',\n",
              "  'Mikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka and Eneko Agirre']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxPZti17CNQx",
        "colab_type": "code",
        "outputId": "25cf494d-474b-467a-852b-16552b768dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(longp)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqL_0bNPCNQ0",
        "colab_type": "code",
        "outputId": "5ec4c616-6f2f-4d7f-d1ac-214e2848687f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "short = read_papers(\"./data/short.txt\")\n",
        "short[:3]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle',\n",
              "  'Shyh-Shiun Hung, Hen-Hsen Huang and Hsin-Hsi Chen'],\n",
              " ['A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers',\n",
              "  'Shen-yun Miao, Chao-Chun Liang and Keh-Yih Su'],\n",
              " ['A Frame-based Sentence Representation for Machine Reading Comprehension',\n",
              "  'Shaoru Guo, Ru Li, Hongye Tan, Xiaoli Li, Yong Guan, Hongyan Zhao and Yueping Zhang']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLNXH4MJCNQ2",
        "colab_type": "code",
        "outputId": "e63030a0-7c2c-47b4-b5bc-11ca29380a25",
        "colab": {}
      },
      "source": [
        "len(short)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrBRORv3CNQ5",
        "colab_type": "code",
        "outputId": "715cc350-82da-4eb2-fd70-986ad051fb0c",
        "colab": {}
      },
      "source": [
        "demo = read_papers(\"./data/demo.txt\")\n",
        "demo[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents',\n",
              "  'Chia-Yu Li, Daniel Ortega, Dirk Väth, Florian Lux, Lindsey Vanderlyn, Maximilian Schmidt, Michael Neumann, Moritz Völkel, Pavel Denisov, Sabrina Jenne, Zorica Kacarevic and Ngoc Thang Vu'],\n",
              " ['BENTO: A Visual Platform for Building Clinical NLP Pipelines Based on CodaLab',\n",
              "  'Yonghao Jin, Fei Li and Hong Yu'],\n",
              " ['Clinical-Coder: Assigning Interpretable ICD-10 Codes to Chinese Clinical Notes',\n",
              "  'Pengfei Cao, Chenwei Yan, xiangling fu, Yubo Chen, Kang Liu, Jun Zhao, Shengping Liu and Weifeng Chong']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GySLkygHCNQ7",
        "colab_type": "code",
        "outputId": "bc0031db-864e-4a43-98d6-4f3c7fa23d47",
        "colab": {}
      },
      "source": [
        "len(demo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cNcBqCsCNQ-",
        "colab_type": "code",
        "outputId": "4d233334-f54a-47aa-e788-dcfec4eabeaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "student = read_papers(\"./data/student.txt\")\n",
        "student[:3]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['#NotAWhore! A Computational Linguistic Perspective of Rape Culture and Victimization on Social Media',\n",
              "  'Ashima Suvarna and Grusha Bhalla'],\n",
              " ['A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples',\n",
              "  'Zhao Meng and Roger Wattenhofer'],\n",
              " ['A Simple and Effective Dependency parser for Telugu',\n",
              "  'Sneha Nallani, Manish Shrivastava and Dipti Sharma']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Zc2x0qCNRA",
        "colab_type": "code",
        "outputId": "49525fec-44d2-4130-8fce-077758220a61",
        "colab": {}
      },
      "source": [
        "len(student)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw4p7l8eCYX2",
        "colab_type": "text"
      },
      "source": [
        "# Sorting by Topic\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c36Fa3LPCovq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fd83c698-ed03-4b8a-f544-86bd403549eb"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "  return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "  result=[]\n",
        "  for token in gensim.utils.simple_preprocess(text) :\n",
        "    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "      result.append(lemmatize_stemming(token))\n",
        "                \n",
        "  return result\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqDjrwyoHEPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FIXME: Better way to get human readable topic names from LDA topics?\n",
        "def list2topiclist(list, num_topics = 8):\n",
        "  processed_docs = []\n",
        "  for line in list:\n",
        "    processed_line = preprocess(line[0])\n",
        "    processed_docs.append(processed_line)\n",
        "\n",
        "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "  \n",
        "  lda = gensim.models.LdaModel(bow_corpus, num_topics, \n",
        "                               id2word = dictionary, passes = 10)\n",
        "\n",
        "\n",
        "  def get_topic_title(idx, topn=3):\n",
        "    topn_terms = [dictionary[x[0]] for x in lda.get_topic_terms(idx, topn)]\n",
        "    return \" \".join(topn_terms)\n",
        "\n",
        "  # Create topic title\n",
        "  list_topic_titles = []\n",
        "  for i in range(num_topics):\n",
        "    list_topic_titles.append(get_topic_title(i))\n",
        "\n",
        "  # Assign list to topic\n",
        "  topic_dict = {}\n",
        "  for line in list:\n",
        "    processed_line = preprocess(line[0])\n",
        "    bow_vector = dictionary.doc2bow(processed_line)\n",
        "    line_topic = sorted(lda.get_document_topics(bow_vector), \n",
        "                        key=lambda tup: tup[1], reverse=True)\n",
        "    topic_title = list_topic_titles[line_topic[0][0]]\n",
        "\n",
        "    if topic_title not in topic_dict:\n",
        "      topic_dict[topic_title] = []\n",
        "\n",
        "    topic_dict[topic_title].append(line)\n",
        "  \n",
        "  return topic_dict\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw8jDBlNNGYH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "de32f3f8-f714-4258-b346-0fb20ba402f9"
      },
      "source": [
        "topic_long = list2topiclist(longp)\n",
        "for topic in topic_long:\n",
        "  print(topic)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['model languag dialogu', 'model languag generat', 'languag relat natur', 'machin translat learn', 'cross neural pars', 'generat semant question', 'generat learn model', 'languag word extract']\n",
            "languag relat natur\n",
            "generat semant question\n",
            "cross neural pars\n",
            "languag word extract\n",
            "generat learn model\n",
            "model languag generat\n",
            "model languag dialogu\n",
            "machin translat learn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUjlRSPtCNRD",
        "colab_type": "text"
      },
      "source": [
        "# Search arXiv Link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws04-fbRCNRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googlesearch import search\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "from difflib import SequenceMatcher\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def similarity(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "\n",
        "def search_arxiv_link(title, sleep=1):\n",
        "    time.sleep(sleep)\n",
        "    link = None\n",
        "    for j in search(title, tld=\"co.in\", num=10, stop=1, pause=0.5):\n",
        "        if 'arxiv.org/abs' in j:\n",
        "            thepage = urllib.request.urlopen(j)\n",
        "            soup = BeautifulSoup(thepage, \"html.parser\")\n",
        "            searched_title = ' '.join(soup.title.text.lower().split()[1:])\n",
        "            if similarity(title, searched_title) > 0.8:\n",
        "                link = j\n",
        "                break\n",
        "            else:\n",
        "                print(\"NOT MATCHED\")\n",
        "                print(title)\n",
        "                print(searched_title)\n",
        "    return link"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAJu9189CNRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_paper_list_with_arxiv_link(f, papers):\n",
        "    for p in tqdm(papers):\n",
        "        title, authors = p\n",
        "        link = search_arxiv_link(title.lower())\n",
        "        if link:\n",
        "            f.write(f\"- {title} [[arXiv]]({link})\\n\")\n",
        "        else:\n",
        "            f.write(f\"- {title}\\n\")\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "796iR6xmLnMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_paper_list_with_arxiv_link_topic(f, papers):\n",
        "  topic_papers = list2topiclist(papers)\n",
        "  print(topic_papers)\n",
        "  for topic in topic_papers:\n",
        "    f.write(\"###\" + topic + \"\\n\")\n",
        "    generate_paper_list_with_arxiv_link(f, topic_papers[topic])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11VvQQBRMZjY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce10b363-8c75-42d2-d8ed-e83dadd34b53"
      },
      "source": [
        "with open(\"papers_with_arxiv_link_topic.md\", \"w\") as f:\n",
        "  f.write(\"## Long Papers\\n\\n\")\n",
        "  generate_paper_list_with_arxiv_link_topic(f, longp)\n",
        "  f.write(\"## Short Papers\\n\\n\")\n",
        "  generate_paper_list_with_arxiv_link_topic(f, short)\n",
        "  f.write(\"## System Demonstrations\\n\\n\")\n",
        "  generate_paper_list_with_arxiv_link_topic(f, demo)\n",
        "  f.write(\"## Student Research Workshop\\n\\n\")\n",
        "  generate_paper_list_with_arxiv_link_topic(f, student)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['base model graph', 'supervis model languag', 'semant learn cross', 'languag natur generat', 'text learn question', 'multi generat domain', 'summar model dialogu', 'machin translat neural']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'machin translat neural': [['2kenize: Tying Subword Sequences for Chinese Script Conversion', 'Pranav A and Isabelle Augenstein'], ['A Formal Hierarchy of RNN Architectures', 'William Merrill, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah A. Smith and Eran Yahav'], ['A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation', 'Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, Zhengyuan Yang, Jie Zhou and Jiebo Luo'], ['A Reinforced Generation of Adversarial Examples for Neural Machine Translation', 'Wei Zou, Shujian Huang, Jun Xie, Xinyu Dai and Jiajun Chen'], ['A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction', 'Yilin Niu, Fangkai Jiao, Mantong Zhou, Ting Yao, Jingfang Xu and Minlie Huang'], ['A Study of Non-autoregressive Model for Sequence Generation', 'Yi Ren, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng Zhao and Tie-Yan Liu'], ['A Tale of Two Perplexities: Sensitivity of Neural Language Models to Lexical Retrieval Deficits in Dementia of the Alzheimer’s Type', 'Trevor Cohen and Serguei Pakhomov'], ['Automatic Detection of Generated Text is Easiest when Humans are Fooled', 'Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch and Douglas Eck'], ['Balancing Training for Multilingual Neural Machine Translation', 'Xinyi Wang, Yulia Tsvetkov and Graham Neubig'], ['Boosting Neural Machine Translation with Similar Translations', 'Jitao XU, Josep Crego and Jean Senellart'], ['BPE-Dropout: Simple and Effective Subword Regularization', 'Ivan Provilkov, Dmitrii Emelianenko and Elena Voita'], ['Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell', 'Djamé Seddah, Farah Essaidi, Amal Fethi, Matthieu Futeral, Benjamin Muller, Pedro Javier Ortiz Suárez, Benoît Sagot and Abhishek Srivastava'], ['Continual Relation Learning via Episodic Memory Activation and Reconsolidation', 'Xu Han, Yi Dai, Tianyu Gao, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun and Jie Zhou'], ['Curriculum Pre-training for End-to-End Speech Translation', 'Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou and Zhenglu Yang'], ['Distilling Annotations via Active Imitation Learning', 'Kianté Brantley, Hal Daumé III and Amr Sharaf'], ['Distilling Knowledge Learned in BERT for Text Generation', 'Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu and Jingjing Liu'], ['Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension', 'Bo Zheng, Haoyang Wen, Yaobo Liang, Nan Duan, Wanxiang Che, Daxin Jiang, Ming Zhou and Ting Liu'], ['Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation', 'Xuanli He, Gholamreza Haffari and Mohammad Norouzi'], ['Effective Estimation of Deep Generative Language Models', 'Tom Pelsmaeker and Wilker Aziz'], ['Efficient Pairwise Annotation of Argument Quality', 'Lukas Gienapp, Benno Stein, Matthias Hagen and Martin Potthast'], ['Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension', 'Fei Yuan, Linjun Shou, Xuanyu Bai, Ming Gong, Yaobo Liang, Nan Duan, Yan Fu and Daxin Jiang'], ['Evaluating Explanation Methods for Neural Machine Translation', 'Jierui Li, Lemao Liu, Huayang Li, Guanlin Li, Guoping Huang and Shuming Shi'], ['Examining the State-of-the-Art in News Timeline Summarization', 'Demian Gholipour Ghalandari and Georgiana Ifrim'], ['Exploiting the Syntax-Model Consistency for Neural Relation Extraction', 'Amir Pouran Ben Veyseh, Franck Dernoncourt, Dejing Dou and Thien Huu Nguyen'], ['FastBERT: a Self-distilling BERT with Adaptive Inference Time', 'Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng and QI JU'], ['Graph Neural News Recommendation with Unsupervised Preference Disentanglement', 'Linmei Hu, Siyong Xu, Chen Li, Cheng Yang, Chuan Shi, Nan Duan, Xing Xie and Ming Zhou'], ['Hard-Coded Gaussian Attention for Neural Machine Translation', 'Weiqiu You, Simeng Sun and Mohit Iyyer'], ['Harnessing the linguistic signal to predict scalar inferences', 'Sebastian Schuster, Yuxing Chen and Judith Degen'], ['Hierarchical Modeling for User Personality Prediction: The Role of Message-Level Attention', 'Veronica Lynn, Niranjan Balasubramanian and H. Andrew Schwartz'], ['Improving Chinese Word Segmentation with Wordhood Memory Networks', 'Yuanhe Tian, Yan Song, Fei Xia, Tong Zhang and Yonggang Wang'], ['Improving Disfluency Detection by Self-Training a Self-Attentive Model', 'Paria Jamshid Lou and Mark Johnson'], ['Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation', 'Biao Zhang, Philip Williams, Ivan Titov and Rico Sennrich'], ['Improving Neural Machine Translation with Soft Template Prediction', 'Jian Yang, Shuming Ma, Dongdong Zhang, Zhoujun Li and Ming Zhou'], ['In Neural Machine Translation, What Does Transfer Learning Transfer?', 'Alham Fikri Aji, Nikolay Bogoychev, Kenneth Heafield and Rico Sennrich'], ['Investigating the effect of auxiliary objectives for the automated grading of learner English speech transcriptions', 'Hannah Craighead, Andrew Caines, Paula Buttery and Helen Yannakoudakis'], ['It’s Morphin’ Time! Combating Linguistic Discrimination with Inflectional Perturbations', 'Samson Tan, Shafiq Joty, Min-Yen Kan and Richard Socher'], ['Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-way Attentions of Auto-analyzed Knowledge', 'Yuanhe Tian, Yan Song, Xiang Ao, Fei Xia, Xiaojun Quan, Tong Zhang and Yonggang Wang'], ['Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation', 'Junliang Guo, Linli Xu and Enhong Chen'], ['Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation', 'Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita and Tiejun Zhao'], ['Learning Efficient Dialogue Policy from Demonstrations through Shaping', 'Huimin Wang, Baolin Peng and Kam-Fai Wong'], ['Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation', 'Qiu Ran, Yankai Lin, Peng Li and Jie Zhou'], ['Location Attention for Extrapolation to Longer Sequences', 'Yann Dubois, Gautier Dagan, Dieuwke Hupkes and Elia Bruni'], ['Machine Reading of Historical Events', 'Or Honovich, Lucas Torroba Hennigen, Omri Abend and Shay B. Cohen'], ['MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices', 'Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang and Denny Zhou'], ['Multi-Label and Multilingual News Framing Analysis', 'Afra Feyza Akyürek, Lei Guo, Randa Elanwar, Prakash Ishwar, Margrit Betke and Derry Tanti Wijaya'], ['Multiscale Collaborative Deep Models for Neural Machine Translation', 'Xiangpeng Wei, Heng Yu, Yue Hu, Yue Zhang, Rongxiang Weng and Weihua Luo'], ['Negative Training for Neural Dialogue Response Generation', 'Tianxing He and James Glass'], ['Neural Mixed Counting Models for Dispersed Topic Discovery', 'Jiemin Wu, Yanghui Rao, Zusheng Zhang, Haoran Xie, Qing Li, Fu Lee Wang and Ziye Chen'], ['Neural Topic Modeling with Bidirectional Adversarial Training', 'Rui Wang, Xuemeng Hu, Deyu Zhou, Yulan He, Yuxuan Xiong, Chenchen Ye and Haiyang Xu'], ['On the Inference Calibration of Neural Machine Translation', 'Shuo Wang, Zhaopeng Tu, Shuming Shi and Yang Liu'], ['On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation', 'Wei Zhao, Goran Glavaš, Maxime Peyrard, Yang Gao, Robert West and Steffen Eger'], ['Parallel Corpus Filtering via Pre-trained Language Models', 'Boliang Zhang, Ajay Nagesh and Kevin Knight'], ['Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT', 'Zhiyong Wu, Yun Chen, Ben Kao and Qun Liu'], ['PeTra: A Sparsely Supervised Memory Model for People Tracking', 'Shubham Toshniwal, Allyson Ettinger, Kevin Gimpel and Karen Livescu'], ['Phone Features Improve Speech Translation', 'Elizabeth Salesky and Alan W Black'], ['Predicting the Focus of Negation: Model and Error Analysis', 'Md Mosharaf Hossain, Kathleen Hamilton, Alexis Palmer and Eduardo Blanco'], ['Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning', 'Alexandre Tamborrino, Nicola Pellicanò, Baptiste Pannier, Pascal Voitot and Louise Naudin'], ['Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension', 'Hongyu Gong, Yelong Shen, Dian Yu, Jianshu Chen and Dong Yu'], ['Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation', 'Xinting Huang, Jianzhong Qi, Yu Sun and Rui Zhang'], ['Simplify the Usage of Lexicon in Chinese NER', 'Ruotian Ma, Minlong Peng, Qi Zhang, Zhongyu Wei and Xuanjing Huang'], ['Spelling Error Correction with Soft-Masked BERT', 'Shaohua Zhang, Haoran Huang, Jicong Liu and Hang Li'], ['STARC: Structured Annotations for Reading Comprehension', 'Yevgeni Berzak, Jonathan Malmaud and Roger Levy'], ['Stock Embeddings Acquired from News Articles and Price History, and an Application to Portfolio Optimization', 'Xin Du and Kumiko Tanaka-Ishii'], ['Structure-Level Knowledge Distillation For Multilingual Sequence Labeling', 'Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Fei Huang and Kewei Tu'], ['Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics', 'Nitika Mathur, Timothy Baldwin and Trevor Cohn'], ['Tchebycheff Procedure for Multi-task Text Classification', 'Yuren Mao, Shuang Yun, Weiwei Liu and Bo Du'], ['Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates', 'Katherine Keith, David Jensen and Brendan O’Connor'], ['The Paradigm Discovery Problem', 'Alexander Erdmann, Micha Elsner, Shijie Wu, Ryan Cotterell and Nizar Habash'], ['Toxicity Detection: Does Context Really Matter?', 'John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon, Nithum Thain and Ion Androutsopoulos'], ['Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification', 'Guangfeng Yan, Lu Fan, Qimai Li, Han Liu, Xiaotong Zhang, Xiao-Ming Wu and Albert Y.S. Lam'], ['“Who said it, and Why?” Provenance for Natural Language Claims', 'Yi Zhang, Zachary Ives and Dan Roth']], 'supervis model languag': [['A Batch Normalized Inference Network Keeps the KL Vanishing Away', 'Qile Zhu, Wei Bi, Xiaojiang Liu, Xiyao Ma, Xiaolin Li and Dapeng Wu'], ['A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization', 'Dongfang Xu, Zeyu Zhang and Steven Bethard'], ['A Systematic Assessment of Syntactic Generalization in Neural Language Models', 'Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox and Roger Levy'], ['An Effectiveness Metric for Ordinal Classification: Formal Properties and Experimental Results', 'Enrique Amigo, Julio Gonzalo, Stefano Mizzaro and Jorge Carrillo-de-Albornoz'], ['Analyzing analytical methods: The case of phonology in neural models of spoken language', 'Grzegorz Chrupała, Bertrand Higy and Afra Alishahi'], ['BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance', 'Timo Schick and Hinrich Schütze'], ['Biomedical Entity Representations with Synonym Marginalization', 'Mujeen Sung, Hwisang Jeon, Jinhyuk Lee and Jaewoo Kang'], ['Bootstrapping Techniques for Polysynthetic Morphological Analysis', 'William Lane and Steven Bird'], ['Bridging Anaphora Resolution as Question Answering', 'Yufang Hou'], ['Contextualized Weak Supervision for Text Classification', 'Dheeraj Mekala and Jingbo Shang'], ['Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA', 'Hyounghun Kim, Zineng Tang and Mohit Bansal'], ['DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking', 'Christopher Hidey, Tuhin Chakrabarty, Tariq Alhindi, Siddharth Varia, Kriste Krstovski, Mona Diab and Smaranda Muresan'], ['Dialogue Coherence Assessment Without Explicit Dialogue Act Labels', 'Mohsen Mesgar, Sebastian Bücker and Iryna Gurevych'], ['Differentiable Window for Dynamic Local Attention', 'Thanh-Tung Nguyen, Xuan-Phi Nguyen, Shafiq Joty and Xiaoli Li'], ['Do Neural Language Models Show Preferences for Syntactic Formalisms?', 'Artur Kulmizev, Vinit Ravishankar, Mostafa Abdou and Joakim Nivre'], ['Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation', 'Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Rajani, Bryan McCann, Vicente Ordonez and Caiming Xiong'], ['Exclusive Hierarchical Decoding for Deep Keyphrase Generation', 'Wang Chen, Hou Pong Chan, Piji Li and Irwin King'], ['Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen', 'Yixin Cao, Ruihao Shui, Liangming Pan, Min-Yen Kan, Zhiyuan Liu and Tat-Seng Chua'], ['Generating Counter Narratives against Online Hate Speech: Data and Strategies', 'Serra Sinem Tekiroğlu, Yi-Ling Chung and Marco Guerini'], ['Handling Rare Entities for Neural Sequence Labeling', 'Yangming Li, Han Li, Kaisheng Yao and Xiaolong Li'], ['Hierarchical Entity Typing via Multi-level Learning to Rank', 'Tongfei Chen, Yunmo Chen and Benjamin Van Durme'], ['How Does Selective Mechanism Improve Self-Attention Networks?', 'Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu and Zhaopeng Tu'], ['Improving Adversarial Text Generation by Modeling the Distant Future', 'Ruiyi Zhang, Changyou Chen, Zhe Gan, Wenlin Wang, Dinghan Shen, Guoyin Wang, Zheng Wen and Lawrence Carin'], ['Improving Truthfulness of Headline Generation', 'Kazuki Matsumaru, Sho Takase and Naoaki Okazaki'], ['INFOTABS: Inference on Tables as Semi-structured Data', 'Vivek Gupta, Maitrey Mehta, Pegah Nokhiz and Vivek Srikumar'], ['Investigating Word-Class Distributions in Word Vector Spaces', 'Ryohei Sasano and Anna Korhonen'], ['It Takes Two to Lie: One to Lie, and One to Listen', 'Denis Peskov, Benny Cheng, Ahmed Elgohary, Joe Barrow, Cristian Danescu-Niculescu-Mizil and Jordan Boyd-Graber'], ['Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in Japanese', 'Tatsuki Kuribayashi, Takumi Ito, Jun Suzuki and Kentaro Inui'], ['Learning Dialog Policies from Weak Demonstrations', 'Gabriel Gordon-Hall, Philip John Gorinski and Shay B. Cohen'], ['Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling', 'Ouyu Lan, Xiao Huang, Bill Yuchen Lin, He Jiang, Liyuan Liu and Xiang Ren'], ['Learning to Segment Actions from Observation and Narration', 'Daniel Fried, Jean-Baptiste Alayrac, Phil Blunsom, Chris Dyer, Stephen Clark and Aida Nematzadeh'], ['MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification', 'Jiaao Chen, Zichao Yang and Diyi Yang'], ['Modeling Code-Switch Languages Using Bilingual Parallel Corpus', 'Grandee Lee and Haizhou Li'], ['Modeling Morphological Typology for Unsupervised Learning of Language Morphology', 'Hongzhi Xu, Jordan Kodner, Mitchell Marcus and Charles Yang'], ['Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts', 'Alex Rinaldi, Jean Fox Tree and Snigdha Chaturvedi'], ['Premise Selection in Natural Language Mathematical Texts', 'Deborah Ferreira and André Freitas'], ['Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering', 'Hao Cheng, Ming-Wei Chang, Kenton Lee and Kristina Toutanova'], ['Rationalizing Medical Relation Prediction from Corpus-level Statistics', 'Zhen Wang, Jennifer Lee, Simon Lin and Huan Sun'], ['Refer360° : A Referring Expression Recognition Dataset in 360° Images', 'Volkan Cirik, Taylor Berg-Kirkpatrick and Louis-Philippe Morency'], ['Robust Encodings: A Framework for Combating Adversarial Typos', 'Erik Jones, Robin Jia, Aditi Raghunathan and Percy Liang'], ['schuBERT: Optimizing Elements of BERT', 'Ashish Khetan and Zohar Karnin'], ['Semi-supervised Contextual Historical Text Normalization', 'Peter Makarov and Simon Clematide'], ['Semi-Supervised Semantic Dependency Parsing Using CRF Autoencoders', 'Zixia Jia, Youmi Ma, Jiong Cai and Kewei Tu'], ['SenseBERT: Driving Some Sense into BERT', 'Yoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua and Yoav Shoham'], ['SeqVAT: Virtual Adversarial Training for Semi-Supervised Sequence Labeling', 'Luoxin Chen, Weitong Ruan, Xinyue Liu and Jianhua Lu'], ['TaPas: Weakly Supervised Table Parsing via Pre-training', 'Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Eisenschlos'], ['To Test Machine Comprehension, Start by Defining Comprehension', 'Jesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen Rambow, Jennifer Chu-Carroll and Dave Ferrucci'], ['Toward Gender-Inclusive Coreference Resolution', 'Yang Trista Cao and Hal Daumé III'], ['Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints', 'Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu and Changyou Chen'], ['When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?', 'Kenneth Joseph and Jonathan Morgan']], 'semant learn cross': [['A Call for More Rigor in Unsupervised Cross-lingual Learning', 'Mikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka and Eneko Agirre'], ['A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking', 'Yong Shan, Zekang Li, Jinchao Zhang, Fandong Meng, Yang Feng, Cheng Niu and Jie Zhou'], ['A Mixture of h − 1 Heads is Better than h Heads', 'Hao Peng, Roy Schwartz, Dianqi Li and Noah A. Smith'], ['A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages', 'Pedro Javier Ortiz Suárez, Laurent Romary and Benoît Sagot'], ['A Top-down Neural Architecture towards Text-level Parsing of Discourse Rhetorical Structure', 'Longyin Zhang, Yuqing Xing, Fang Kong, Peifeng Li and Guodong Zhou'], ['Analyzing Political Parody in Social Media', 'Antonios Maronikolakis, Danae Sánchez Villegas, Daniel Preotiuc-Pietro and Nikolaos Aletras'], ['Asking and Answering Questions to Evaluate the Factual Consistency of Summaries', 'Alex Wang, Kyunghyun Cho and Mike Lewis'], ['Balancing Objectives in Counseling Conversations: Advancing Forwards or Looking Backwards', 'Justine Zhang and Cristian Danescu-Niculescu-Mizil'], ['Benchmarking Multimodal Regex Synthesis with Complex Structures', 'Xi Ye, Qiaochu Chen, Isil Dillig and Greg Durrett'], ['CamemBERT: a Tasty French Language Model', 'Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric de la Clergerie, Djamé Seddah and Benoît Sagot'], ['ChartDialogs: Plotting from Natural Language Instructions', 'Yutong Shao and Ndapa Nakashole'], ['CraftAssist Instruction Parsing: Semantic Parsing for a Voxel-World Assistant', 'Kavya Srinet, Yacine Jernite, Jonathan Gray and Arthur Szlam'], ['Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus', 'Hao Fei, Meishan Zhang and Donghong Ji'], ['Cross-Lingual Unsupervised Sentiment Classification with Multi-View Transfer Learning', 'Hongliang Fei and Ping Li'], ['Cross-media Structured Common Space for Multimedia Event Extraction', 'Manling Li, Alireza Zareian, Qi Zeng, Spencer Whitehead, Di Lu, Heng Ji and Shih-Fu Chang'], ['Cross-Modality Relevance for Reasoning on Language and Vision', 'Chen Zheng, Quan Guo and Parisa Kordjamshidi'], ['Data Manipulation: Towards Effective Instance Learning for Neural Dialogue Generation via Learning to Augment and Reweight', 'Hengyi Cai, Hongshen Chen, Yonghao Song, Cheng Zhang, Xiaofang Zhao and Dawei Yin'], ['Efficient Constituency Parsing by Pointing', 'Thanh-Tung Nguyen, Xuan-Phi Nguyen, Shafiq Joty and Xiaoli Li'], ['Efficient Dialogue State Tracking by Selectively Overwriting Memory', 'Sungdong Kim, Sohee Yang, Gyuwan Kim and Sang-Woo Lee'], ['ESPRIT: Explaining Solutions to Physical Reasoning Tasks', 'Nazneen Fatema Rajani, Rui Zhang, Yi Chern Tan, Stephan Zheng, Jeremy Weiss, Aadit Vyas, Abhijit Gupta, Caiming Xiong, Richard Socher and Dragomir Radev'], ['Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer', 'Chulun Zhou, Liangyu Chen, Jiachen Liu, Xinyan Xiao, Jinsong Su, Sheng Guo and Hua Wu'], ['Extracting Headless MWEs from Dependency Parse Trees: Parsing, Tagging, and Joint Modeling Approaches', 'Tianze Shi and Lillian Lee'], ['Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning', 'Joongbo Shin, Yoonhyung Lee, Seunghyun Yoon and Kyomin Jung'], ['Fast and Accurate Non-Projective Dependency Tree Linearization', 'Xiang Yu, Simon Tannert, Ngoc Thang Vu and Jonas Kuhn'], ['From English to Code-Switching: Transfer Learning with Strong Morphological Clues', 'Gustavo Aguilar and Thamar Solorio'], ['From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains', 'Jan-Christoph Klie, Richard Eckart de Castilho and Iryna Gurevych'], ['GLUECoS: An Evaluation Benchmark for Code-Switched NLP', 'Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram and Monojit Choudhury'], ['He said “who’s gonna take care of your children when you are at ACL?”: Reported Sexist Acts are Not Sexist', 'Patricia Chiril, Véronique Moriceau, Farah Benamara, Alda Mari, Gloria Origgi and Marlène Coulomb-Gully'], ['Joint Diacritization, Lemmatization, Normalization, and Fine-Grained Morphological Tagging', 'Nasser Zalmout and Nizar Habash'], ['KLEJ: Comprehensive Benchmark for Polish Language Understanding', 'Piotr Rybak, Robert Mroczkowski, Janusz Tracz and Ireneusz Gawlik'], ['Learning to Faithfully Rationalize by Construction', 'Sarthak Jain, Sarah Wiegreffe, Yuval Pinter and Byron C. Wallace'], ['Low-Resource Generation of Multi-hop Reasoning Questions', 'Jianxing Yu, Wei Liu, Shuang Qiu, Qinliang Su, Kai Wang, Xiaojun Quan and Jian Yin'], ['MLQA: Evaluating Cross-lingual Extractive Question Answering', 'Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel and Holger Schwenk'], ['Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence', 'Xiaoyu Shen, Ernie Chang, Hui Su, Cheng Niu and Dietrich Klakow'], ['Neural Reranking for Dependency Parsing: An Evaluation', 'Bich-Ngoc Do and Ines Rehbein'], ['Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection', 'Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton and Yoav Goldberg'], ['On the Cross-lingual Transferability of Monolingual Representations', 'Mikel Artetxe, Sebastian Ruder and Dani Yogatama'], ['Paraphrase Generation by Learning How to Edit from Samples', 'Amirhossein Kazemnejad, Mohammadreza Salehi and Mahdieh Soleymani Baghshah'], ['Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association', 'Nan Xu, Zhixiong Zeng and Wenji Mao'], ['Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment', 'Forrest Davis and Marten van Schijndel'], ['Rethinking Dialogue State Tracking with Reasoning', 'Lizi Liao, Yunshan Ma, Wenqiang Lei and Tat-Seng Chua'], ['Screenplay Summarization Using Latent Narrative Structure', 'Pinelopi Papalampidi, Frank Keller, Lea Frermann and Mirella Lapata'], ['ScriptWriter: Narrative-Guided Script Generation', 'Yutao Zhu, Ruihua Song, Zhicheng Dou, Jian-Yun Nie and Jin Zhou'], ['Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation', 'Xabier Soto, Dimitar Shterionov, Alberto Poncelas and Andy Way'], ['Semantic Graphs for Generating Deep Questions', 'Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua and Min-Yen Kan'], ['Semantic Parsing for English as a Second Language', 'Yuanyuan Zhao, Weiwei Sun, Junjie Cao and Xiaojun Wan'], ['Semantic Scaffolds for Pseudocode-to-Code Generation', 'Ruiqi Zhong, Mitchell Stern and Dan Klein'], ['Should All Cross-Lingual Embeddings Speak English?', 'Antonios Anastasopoulos and Graham Neubig'], ['Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language', 'Qianhui Wu, Zijia Lin, Börje Karlsson, Jian-Guang Lou and Biqing Huang'], ['Social Bias Frames: Reasoning about Social and Power Implications of Language', 'Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith and Yejin Choi'], ['Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset', 'Revanth Rameshkumar and Peter Bailey'], ['Structured Tuning for Semantic Role Labeling', 'Tao Li, Parth Anand Jawale, Martha Palmer and Vivek Srikumar'], ['Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation', 'Kaustubh Dhole and Christopher D. Manning'], ['The State and Fate of Linguistic Diversity and Inclusion in the NLP World', 'Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali and Monojit Choudhury'], ['The Summary Loop: Learning to Write Abstractive Summaries Without Examples', 'Philippe Laban, Andrew Hsi, John Canny and Marti A. Hearst'], ['Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation', 'Bo Pang, Erik Nijkamp, Wenjuan Han, Linqi Zhou, Yixian Liu and Kewei Tu'], ['Towards Unsupervised Language Understanding and Generation by Joint Dual Learning', 'Shang-Yu Su, Chao-Wei Huang and Yun-Nung Chen'], ['Universal Decompositional Semantic Parsing', 'Elias Stengel-Eskin, Aaron Steven White, Sheng Zhang and Benjamin Van Durme'], ['Unsupervised Cross-lingual Representation Learning at Scale', 'Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov'], ['Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing', 'Ruisheng Cao, Su Zhu, Chenyu Yang, Chen Liu, Rao Ma, Yanbin Zhao, Lu Chen and Kai Yu'], ['What are the Goals of Distributional Semantics?', 'Guy Emerson'], ['Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking', 'Giovanni Campagna, Agata Foryciarz, Mehrad Moradshahi and Monica Lam']], 'text learn question': [['A Comprehensive Analysis of Preprocessing for Word Representation Learning in Affective Tasks', 'Nastaran Babanejad, Ameeta Agrawal, Aijun An and Manos Papagelis'], ['A Girl Has A Name: Detecting Authorship Obfuscation', 'Asad Mahmood, Zubair Shafiq and Padmini Srinivasan'], ['A Graph Auto-encoder Model of Derivational Morphology', 'Valentin Hofmann, Hinrich Schütze and Janet Pierrehumbert'], ['Aligned Dual Channel Graph Convolutional Network for Visual Question Answering', 'Qingbao Huang, Jielong Wei, Yi Cai, Changmeng Zheng, Junying Chen, Ho-fung Leung and Qing Li'], ['AMR Parsing via Graph-Sequence Iterative Inference', 'Deng Cai and Wai Lam'], ['An Online Semantic-enhanced Dirichlet Model for Short Text Stream Clustering', 'Jay Kumar, Junming Shao, Salah Uddin and Wazir Ali'], ['Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition', 'Paloma Jeretic, Alex Warstadt, Suvrat Bhooshan and Adina Williams'], ['Attentive Pooling with Learnable Norms for Text Representation', 'Chuhan Wu, Fangzhao Wu, Tao Qi, Xiaohui Cui and Yongfeng Huang'], ['Automatic Generation of Citation Texts in Scholarly Papers: A Pilot Study', 'Xinyu Xing, Xiaosheng Fan and Xiaojun Wan'], ['BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps', 'Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene Ie and Fei Sha'], ['BLEURT: Learning Robust Metrics for Text Generation', 'Thibault Sellam, Dipanjan Das and Ankur Parikh'], ['Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation', 'Chao Zhao, Marilyn Walker and Snigdha Chaturvedi'], ['Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data', 'Emily M. Bender and Alexander Koller'], ['Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset', 'Xiang Yue, Bernal Jimenez Gutierrez and Huan Sun'], ['CluBERT: A Cluster-Based Approach for Learning Sense Distributions in Multiple Languages', 'Tommaso Pasini, Federico Scozzafava and Bianca Scarlini'], ['Connecting Embeddings for Knowledge Graph Entity Typing', 'Yu Zhao, anxiang zhang, Ruobing Xie, Kang Liu and Xiaojie WANG'], ['DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering', 'Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian and Niranjan Balasubramanian'], ['Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting', 'Guanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Conghui Zhu and Tiejun Zhao'], ['Dice Loss for Data-imbalanced NLP Tasks', 'Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu and Jiwei Li'], ['Discrete Latent Variable Representations for Low-Resource Text Classification', 'Shuning Jin, Sam Wiseman, Karl Stratos and Karen Livescu'], ['DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim Verification', 'Lianwei Wu, Yuan Rao, Yongqiang Zhao, Hao Liang and Ambreen Nazir'], ['End-to-End Bias Mitigation by Modelling Biases in Corpora', 'Rabeeh Karimi Mahabadi, Yonatan Belinkov and James Henderson'], ['Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?', 'Peter Hase and Mohit Bansal'], ['Exact yet Efficient Graph Parsing, Bi-directional Locality and the Constructivist Hypothesis', 'Yajie Ye and Weiwei Sun'], ['Extractive Summarization as Text Matching', 'Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu and Xuanjing Huang'], ['Fact-based Text Editing', 'Hayate Iso, Chao Qiao and Hang Li'], ['Fine-grained Fact Verification with Kernel Graph Attention Network', 'Zhenghao Liu, Chenyan Xiong, Maosong Sun and Zhiyuan Liu'], ['From SPMRL to NMRL: What Did We Learn (and Unlearn) in a Decade of Parsing Morphologically-Rich Languages (MRLs)?', 'Reut Tsarfaty, Dan Bareket, Stav Klein and Amit Seker'], ['GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media', 'Yi-Ju Lu and Cheng-Te Li'], ['Generating Fact Checking Explanations', 'Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma and Isabelle Augenstein'], ['Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection', 'Hanjie Chen, Guangtao Zheng and Yangfeng Ji'], ['Good-Enough Compositional Data Augmentation', 'Jacob Andreas'], ['How to Ask Good Questions? Try to Leverage Paraphrases', 'Xin Jia, Wenjie Zhou, Xu Sun and Yunfang Wu'], ['In Layman’s Terms: Semi-Open Relation Extraction from Scientific Texts', 'Ruben Kruiper, Julian Vincent, Jessica Chen-Burger, Marc Desmulliez and Ioannis Konstas'], ['Interactive Classification by Asking Informative Questions', 'Lili Yu, Howard Chen, Sida I. Wang, Tao Lei and Yoav Artzi'], ['iSarcasm: A Dataset of Intended Sarcasm', 'Silviu Oprea and Walid Magdy'], ['Iterative Edit-Based Unsupervised Sentence Simplification', 'Dhruv Kumar, Lili Mou, Lukasz Golab and Olga Vechtomova'], ['Learning Interpretable Relationships between Entities, Relations and Concepts via Bayesian Structure Learning on Open Domain Facts', 'Jingyuan Zhang, Mingming Sun, Yue Feng and Ping Li'], ['Learning Source Phrase Representations for Neural Machine Translation', 'Hongfei Xu, Josef van Genabith, Deyi Xiong, Qiuhui Liu and Jingyi Zhang'], ['Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction', 'Zi Chai and Xiaojun Wan'], ['Learning to Deceive with Attention-Based Explanations', 'Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig and Zachary C. Lipton'], ['Learning to execute instructions in a Minecraft dialogue', 'Prashant Jayannavar, Anjali Narayan-Chen and Julia Hockenmaier'], ['Learning to Identify Follow-Up Questions in Conversational Question Answering', 'Souvik Kundu, Qian Lin and Hwee Tou Ng'], ['Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks', 'Yanbin Zhao, Lu Chen, Zhi Chen, Ruisheng Cao, Su Zhu and Kai Yu'], ['LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network', 'Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang, Jiahai Wang and Jian Yin'], ['Measuring Forecasting Skill from Text', 'Shi Zong, Alan Ritter and Eduard Hovy'], ['Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders', 'Terra Blevins and Luke Zettlemoyer'], ['Multimodal Neural Graph Memory Networks for Visual Question Answering', 'Mahmoud Khademi'], ['MultiQT: Multimodal learning for real-time question tracking in speech', 'Jakob D. Havtorn, Jan Latko, Joakim Edin, Lars Maaløe, Lasse Borgholt, Lorenzo Belgrano, Nicolai Jacobsen, Regitze Sdun and Željko Agić'], ['Neighborhood Matching Network for Entity Alignment', 'Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang and Dongyan Zhao'], ['Neural CRF Model for Sentence Alignment in Text Simplification', 'Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong and Wei Xu'], ['On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond', 'Chen Wu, Prince Zizhuang Wang and William Yang Wang'], ['Out of the Echo Chamber: Detecting Countering Debate Speeches', 'Matan Orbach, Yonatan Bilu, Assaf Toledo, Dan Lahav, Michal Jacovi, Ranit Aharonov and Noam Slonim'], ['PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable', 'Siqi Bao, Huang He, Fan Wang, Hua Wu and Haifeng Wang'], ['Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders', 'Yu Duan, Canwen Xu, Jiaxin Pei, Jialong Han and Chenliang Li'], ['Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport', 'Kyle Swanson, Lili Yu and Tao Lei'], ['RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers', 'Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov and Matthew Richardson'], ['Reasoning Over Semantic-Level Graph for Fact Checking', 'Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang and Jian Yin'], ['Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem', 'Danielle Saunders and Bill Byrne'], ['Rigid Formats Controlled Text Generation', 'Piji Li, Haisong Zhang, Xiaojiang Liu and Shuming Shi'], ['RikiNet: Reading Wikipedia Pages for Natural Question Answering', 'Dayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng Chen, Daxin Jiang, Jiancheng Lv and Nan Duan'], ['SimulSpeech: End-to-End Simultaneous Speech to Text Translation', 'Yi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao and Tie-Yan Liu'], ['Span Selection Pre-training for Question Answering', 'Michael Glass, Alfio Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, G P Shrivatsa Bhargav, Dinesh Garg and Avi Sil'], ['Speech Translation and the End-to-End Promise: Taking Stock of Where We Are', 'Matthias Sperber and Matthias Paulik'], ['SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check', 'Xingyi Cheng, Weidi Xu, Kunlong Chen, Shaohua Jiang, Feng Wang, Taifeng Wang, Wei Chu and Yuan Qi'], ['Text-Based Ideal Points', 'Keyon Vafa, Suresh Naidu and David Blei'], ['That is a Known Lie: Detecting Previously Fact-Checked Claims', 'Shaden Shaar, Nikolay Babulkov, Giovanni Da San Martino and Preslav Nakov'], ['Towards Transparent and Explainable Attention Models', 'Akash Kumar Mohankumar, Preksha Nema, Sharan Narasimhan, Mitesh M. Khapra, Balaji Vasan Srinivasan and Balaraman Ravindran'], ['Towards Understanding Gender Bias in Relation Extraction', 'Andrew Gaut, Tony Sun, Shirlyn Tang, Yuxin Huang, Jing Qian, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang and William Yang Wang'], ['Understanding Attention for Text Classification', 'Xiaobing Sun and Wei Lu'], ['Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering', 'Vikas Yadav, Steven Bethard and Mihai Surdeanu'], ['Unsupervised Morphological Paradigm Completion', 'Huiming Jin, Liwei Cai, Yihui Peng, Chen Xia, Arya McCarthy and Katharina Kann'], ['Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting', 'Po-Yao Huang, Junjie Hu, Xiaojun Chang and Alexander Hauptmann'], ['What determines the order of adjectives in English? Comparing efficiency-based theories using dependency treebanks', 'Richard Futrell, William Dyer and Greg Scontras'], ['What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context', 'Ramy Baly, Georgi Karadzhov, Jisun An, Haewoon Kwak, Yoan Dinkov, Ahmed Ali, James Glass and Preslav Nakov'], ['Zero-shot Text Classification via Reinforced Self-training', 'Zhiquan Ye, Yuxia Geng, Jiaoyan Chen, Jingmin Chen, Xiaoxiao Xu, Suhang Zheng, Feng Wang, Jun Zhang and Huajun Chen']], 'multi generat domain': [['A Corpus for Large-Scale Phonetic Typology', 'Elizabeth Salesky, Eleanor Chodroff, Tiago Pimentel, Matthew Wiesner, Ryan Cotterell, Alan W Black and Jason Eisner'], ['Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation', 'Arya D. McCarthy, Xian Li, Jiatao Gu and Ning Dong'], ['Automatic Poetry Generation from Prosaic Text', 'Tim Van de Cruys'], ['Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation', 'Weixin Liang, James Zou and Zhou Yu'], ['Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences', 'Xiangyu Duan, Baijun Ji, Hao Jia, Min Tan, Min Zhang, Boxing Chen, Weihua Luo and Yue Zhang'], ['Can You Put it All Together: Evaluating Conversational Agents’ Ability to Blend Skills', 'Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston and Y-Lan Boureau'], ['CDL: Curriculum Dual Learning for Emotion-Controllable Response Generation', 'Lei Shen and Yang Feng'], ['CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning', 'Alessandro Suglia, Ioannis Konstas, Andrea Vanzo, Emanuele Bastianelli, Desmond Elliott, Stella Frank and Oliver Lemon'], ['Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation', 'Jun Xu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che and Ting Liu'], ['Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage', 'Ashish V. Thapliyal and Radu Soricut'], ['Detecting Perceived Emotions in Hurricane Disasters', 'Shrey Desai, Cornelia Caragea and Junyi Jessy Li'], ['Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness', 'Sixing Wu, Ying Li, Dawei Zhang, Yang Zhou and Zhonghai Wu'], ['Diversifying Dialogue Generation with Non-Conversational Text', 'Hui Su, Xiaoyu Shen, Sanqiang Zhao, Zhou Xiao, Pengwei Hu, Randy Zhong, Cheng Niu and Jie Zhou'], ['Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain', 'Shadi Saleh and Pavel Pecina'], ['Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding', 'Xinya Du and Claire Cardie'], ['DoQA - Accessing Domain-Specific FAQs via Conversational QA', 'Jon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Deriu, Mark Cieliebak and Eneko Agirre'], ['DRTS Parsing with Structure-Aware Encoding and Decoding', 'Qiankun Fu, Yue Zhang, Jiangming Liu and Meishan Zhang'], ['Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog', 'Libo Qin, Xiao Xu, Wanxiang Che, Yue Zhang and Ting Liu'], ['Dynamic Online Conversation Recommendation', 'Xingshan Zeng, Jing Li, Lu Wang, Zhiming Mao and Kam-Fai Wong'], ['Efficient Second-Order TreeCRF for Neural Dependency Parsing', 'Yu Zhang, Zhenghua Li and Min Zhang'], ['Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge', 'Bowen Zhang, Min Yang, Xutao Li, Yunming Ye, Xiaofei Xu and Kuai Dai'], ['Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading', 'Yifan Gao, Chien-Sheng Wu, Shafiq Joty, Caiming Xiong, Richard Socher, Irwin King, Michael Lyu and Steven C.H. Hoi'], ['Explicit Semantic Decomposition for Definition Generation', 'Jiahuan Li, Yu Bao, Shujian Huang, Xinyu Dai and Jiajun Chen'], ['Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing', 'Alane Suhr, Ming-Wei Chang, Peter Shaw and Kenton Lee'], ['Fluent Response Generation for Conversational Question Answering', 'Ashutosh Baheti, Alan Ritter and Kevin Small'], ['Frugal Paradigm Completion', 'Alexander Erdmann, Tom Kenter, Markus Becker and Christian Schallhart'], ['Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus', 'Luisa Bentivogli, Beatrice Savoldi, Matteo Negri, Mattia A. Di Gangi, Roldano Cattoni and Marco Turchi'], ['Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy', 'Xiexiong Lin, Weiyu Jian, Jianshan He, Taifeng Wang and Wei Chu'], ['GoEmotions: A Dataset of Fine-Grained Emotions', 'Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade and Sujith Ravi'], ['Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs', 'Houyu Zhang, Zhenghao Liu, Chenyan Xiong and Zhiyuan Liu'], ['Grounding Conversations with Improvised Dialogues', 'Hyundong Cho and Jonathan May'], ['Guiding Variational Response Generator to Exploit Persona', 'Bowen Wu, Mengyuan Li, Zongsheng Wang, Yifu Chen, Derek F. Wong, Qihang Feng, Junhong Huang and Baoxun Wang'], ['Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation', 'Liting Liu, Jie Liu, Wenzheng Zhang, Ziming Chi, Wenxuan Shi and Yalou Huang'], ['Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?', 'Cansu Sen, Thomas Hartvigsen, Biao Yin, Xiangnan Kong and Elke Rundensteiner'], ['Image-Chat: Engaging Grounded Conversations', 'Kurt Shuster, Samuel Humeau, Antoine Bordes and Jason Weston'], ['Inflecting when there’s no majority: Limitations of encoder-decoder neural networks as cognitive models for German plurals', 'Kate McCurdy, Sharon Goldwater and Adam Lopez'], ['INSET: Sentence Infilling with INter-SEntential Transformer', 'Yichen Huang, Yizhe Zhang, Oussama Elachqar and Yu Cheng'], ['KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis', 'Deepanway Ghosal, Devamanyu Hazarika, Abhinaba Roy, Navonil Majumder, Rada Mihalcea and Soujanya Poria'], ['Large Scale Multi-Actor Generative Dialog Modeling', 'Alex Boyd, Raul Puri, Mohammad Shoeybi, Mostofa Patwary and Bryan Catanzaro'], ['Learning a Multi-Domain Curriculum for Neural Machine Translation', 'Wei Wang, Ye Tian, Jiquan Ngiam, Yinfei Yang, Isaac Caswell and Zarana Parekh'], ['Learning and Evaluating Emotion Lexicons for 91 Languages', 'Sven Buechel, Susanna Rücker and Udo Hahn'], ['Learning Constraints for Structured Prediction Using Rectifier Networks', 'Xingyuan Pan, Maitrey Mehta and Vivek Srikumar'], ['Max-Margin Incremental CCG Parsing', 'Miloš Stanojević and Mark Steedman'], ['MIE: A Medical Information Extractor towards Medical Dialogues', 'Yuanzhe Zhang, Zhongtao Jiang, Tao Zhang, Shiwan Liu, Jiarun Cao, Kang Liu, Shengping Liu and Jun Zhao'], ['MMPE: A Multi-Modal Interface for Post-Editing Machine Translation', 'Nico Herbig, Tim Düwel, Santanu Pal, Kalliopi Maria Meladaki, Mahsa Monshizadeh, Antonio Krüger and Josef van Genabith'], ['Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning', 'Angeliki Lazaridou, Anna Potapenko and Olivier Tieleman'], ['Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition', 'Ryuichi Takanobu, Runze Liang and Minlie Huang'], ['Multi-Cell Compositional LSTM for NER Domain Adaptation', 'Chen Jia and Yue Zhang'], ['Multi-Domain Dialogue Acts and Response Co-Generation', 'Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan and Jianxing Yu'], ['Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference', 'Jing Wang, Mayank Kulkarni and Daniel Preotiuc-Pietro'], ['Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing', 'Haoming Jiang, Chen Liang, Chong Wang and Tuo Zhao'], ['Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization', 'Hanqi Jin, Tianming Wang and Xiaojun Wan'], ['Multi-Hypothesis Machine Translation Evaluation', 'Marina Fomicheva, Lucia Specia and Francisco Guzmán'], ['Multi-Sentence Argument Linking', 'Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins and Benjamin Van Durme'], ['Neural Generation of Dialogue Response Timings', 'Matthew Roddy and Naomi Harte'], ['Obtaining Faithful Interpretations from Compositional Neural Networks', 'Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant and Matt Gardner'], ['On The Evaluation of Machine Translation SystemsTrained With Back-Translation', 'Sergey Edunov, Myle Ott, Marc’Aurelio Ranzato and Michael Auli'], ['One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases', 'Xingdi Yuan, Tong Wang, Rui Meng, Khushboo Thaker, Peter Brusilovsky, Daqing He and Adam Trischler'], ['Paraphrase Augmented Task-Oriented Dialog Generation', 'Silin Gao, Yichi Zhang, Zhijian Ou and Zhou Yu'], ['Phonetic and Visual Priors for Decipherment of Informal Romanization', 'Maria Ryskina, Matthew R. Gormley and Taylor Berg-Kirkpatrick'], ['PuzzLing Machines: A Challenge on Learning From Small Data', 'Gözde Gül Şahin, Yova Kementchedjhieva, Phillip Rust and Iryna Gurevych'], ['Representation Learning for Information Extraction from Form-like Documents', 'Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley Wendt, Qi Zhao and Marc Najork'], ['Response-Anticipated Memory for On-Demand Knowledge Integration in Response Generation', 'Zhiliang Tian, Wei Bi, Dongkyu Lee, Lanqing Xue, Yiping Song, Xiaojiang Liu and Nevin L. Zhang'], ['SAS: Dialogue State Tracking via Slot Attention and Slot Information Sharing', 'Jiaying Hu, Yan Yang, Chencai Chen, Liang He and Zhou Yu'], ['SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations', 'Xiang Kong, Varun Gangal and Eduard Hovy'], ['SciREX: A Challenge Dataset for Document-Level Information Extraction', 'Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi and Iz Beltagy'], ['Selective Question Answering under Domain Shift', 'Amita Kamath, Robin Jia and Percy Liang'], ['Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis', 'Dushyant Singh Chauhan, Dhanush S R, Asif Ekbal and Pushpak Bhattacharyya'], ['Speaker Sensitive Response Evaluation Model', 'JinYeong Bak and Alice Oh'], ['Speakers enhance contextually confusable words', 'Eric Meinhardt, Eric Bakovic and Leon Bergen'], ['SPECTER: Document-level Representation Learning using Citation-informed Transformers', 'Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey and Daniel Weld'], ['The Cascade Transformer: an Application for Efficient Answer Sentence Selection', 'Luca Soldaini and Alessandro Moschitti'], ['The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents', 'Kurt Shuster, Da JU, Stephen Roller, Emily Dinan, Y-Lan Boureau and Jason Weston'], ['The SOFC-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain', 'Annemarie Friedrich, Heike Adel, Federico Tomazic, Johannes Hingerl, Renou Benteau, Anika Marusczyk and Lukas Lange'], ['To Boldly Query What No One Has Annotated Before? The Frontiers of Corpus Querying', 'Markus Gärtner and Kerstin Jung'], ['Towards Conversational Recommendation over Multi-Type Dialogs', 'Zeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che and Ting Liu'], ['Towards Debiasing Sentence Representations', 'Paul Pu Liang, Irene Mengze Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov and Louis-Philippe Morency'], ['Towards Emotion-aided Multi-modal Dialogue Act Classification', 'Tulika Saha, Aditya Patra, Sriparna Saha and Pushpak Bhattacharyya'], ['Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs', 'Jun Chen, Xiaoya Dai, Quan Yuan, Chao Lu and Haifeng Huang'], ['TVQA+: Spatio-Temporal Grounding for Video Question Answering', 'Jie Lei, Licheng Yu, Tamara Berg and Mohit Bansal'], ['TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories', 'Giannis Karamanolakis, Jun Ma and Xin Luna Dong'], ['Uncertainty-Aware Curriculum Learning for Neural Machine Translation', 'Yikai Zhou, Baosong Yang, Derek F. Wong, Yu Wan and Lidia S. Chao'], ['WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge', 'Hongming Zhang, Xinran Zhao and Yangqiu Song'], ['You Impress Me: Dialogue Generation via Mutual Persona Perception', 'Qian Liu, Yihong Chen, Bei Chen, Jian-Guang Lou, Zixuan Chen, Bin Zhou and Dongmei Zhang']], 'languag natur generat': [['A Generative Model for Joint Natural Language Understanding and Generation', 'Bo-Hsiang Tseng, Jianpeng Cheng, Yimai Fang and David Vandyke'], ['Adversarial NLI: A New Benchmark for Natural Language Understanding', 'Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston and Douwe Kiela'], ['Agreement Prediction of Arguments in Cyber Argumentation for Detecting Stance Polarity and Intensity', 'Joseph Sirrianni, Xiaoqing Liu and Douglas Adams'], ['Analysing Lexical Semantic Change with Contextualised Word Representations', 'Mario Giulianelli, Marco Del Tredici and Raquel Fernández'], ['Autoencoding Pixies: Amortised Variational Inference with Graph Convolutions for Functional Distributional Semantics', 'Guy Emerson'], ['Automated Evaluation of Writing – 50 Years and Counting', 'Beata Beigman Klebanov and Nitin Madnani'], ['BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension', 'Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov and Luke Zettlemoyer'], ['BiRRE: Learning Bidirectional Residual Relation Embeddings for Supervised Hypernymy Detection', 'Chengyu Wang and Xiaofeng He'], ['Calibrating Structured Output Predictors for Natural Language Processing', 'Abhyuday Jagannatha and Hong Yu'], ['CorefQA: Coreference Resolution as Query-based Span Prediction', 'Wei Wu, Fei Wang, Arianna Yuan, Fei Wu and Jiwei Li'], ['Cross-Linguistic Syntactic Evaluation of Word Prediction Models', 'Aaron Mueller, Garrett Nicolai, Panayiota Petrou-Zeniou, Natalia Talmina and Tal Linzen'], ['Cross-modal Coherence Modeling for Caption Generation', 'Malihe Alikhani, Piyush Sharma, Shengjie Li, Radu Soricut and Matthew Stone'], ['Curriculum Learning for Natural Language Understanding', 'Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie and Yongdong Zhang'], ['Distinguish Confusing Law Articles for Legal Judgment Prediction', 'Nuo Xu, Pinghui Wang, Long Chen, Li Pan, Xiaoyan Wang and Junzhou Zhao'], ['Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?', 'Hitomi Yanaka, Koji Mineshima, Daisuke Bekki and Kentaro Inui'], ['Emergence of Syntax Needs Minimal Supervision', 'Raphaël Bailly and Kata Gábor'], ['Empowering Active Learning to Jointly Optimize System and User Demands', 'Ji-Ung Lee, Christian M. Meyer and Iryna Gurevych'], ['Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder', 'Daya Guo, Duyu Tang, Nan Duan, Jian Yin, Daxin Jiang and Ming Zhou'], ['Examining Citations of Natural Language Processing Literature', 'Saif M. Mohammad'], ['Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach', 'Wenyu DU, Zhouhan Lin, Yikang Shen, Timothy J. O’Donnell, Yoshua Bengio and Yue Zhang'], ['Facet-Aware Evaluation for Extractive Summarization', 'Yuning Mao, Liyuan Liu, Qi Zhu, Xiang Ren and Jiawei Han'], ['Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences', 'Dmitry Nikolaev, Ofir Arviv, Taelin Karidi, Neta Kenneth, Veronika Mitnik, Lilja Maria Saeboe and Omri Abend'], ['From Arguments to Key Points: Towards Automatic Argument Summarization', 'Roy Bar-Haim, Lilach Eden, Roni Friedman, Yoav Kantor, Dan Lahav and Noam Slonim'], ['Gated Convolutional Bidirectional Attention-based Model for Off-topic Spoken Response Detection', 'Yefei Zha, Ruobing Li and Hui Lin'], ['Gender Gap in Natural Language Processing Research: Disparities in Authorship and Citations', 'Saif M. Mohammad'], ['Generalizing Natural Language Analysis through Span-relation Representations', 'Zhengbao Jiang, Wei Xu, Jun Araki and Graham Neubig'], ['HAT: Hardware-Aware Transformers for Efficient Natural Language Processing', 'Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan and Song Han'], ['Highway Transformer: Self-Gating Enhanced Self-Attentive Networks', 'Yekun Chai, Shuo Jin and Xinwen Hou'], ['How does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope', 'Yiyun Zhao and Steven Bethard'], ['HyperCore: Hyperbolic and Co-graph Representation for Automatic ICD Coding', 'Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Shengping Liu and Weifeng Chong'], ['Improved Natural Language Generation via Loss Truncation', 'Daniel Kang and Tatsunori Hashimoto'], ['Improving Disentangled Text Representation Learning with Information-Theoretic Guidance', 'Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong Li and Lawrence Carin'], ['Improving Image Captioning with Better Use of Caption', 'Zhan Shi, Xu Zhou, Xipeng Qiu and Xiaodan Zhu'], ['Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models', 'Kaiji Lu, Piotr Mardziel, Klas Leino, Matt Fredrikson and Anupam Datta'], ['Integrating Semantic and Structural Information with Graph Convolutional Network for Controversy Detection', 'Lei Zhong, Juan Cao, Qiang Sheng, Junbo Guo and Ziang Wang'], ['Interactive Construction of User-Centric Dictionary for Text Analytics', 'Ryosuke Kohita, Issei Yoshida, Hiroshi Kanayama and Tetsuya Nasukawa'], ['Language (Re)modelling: Towards Embodied Language Understanding', 'Ronen Tamari, Chen Shani, Tom Hope, Miriam R L Petruck, Omri Abend and Dafna Shahaf'], ['Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions', 'Tian Jin, Zhun Liu, Shengjia Yan, Alexandre Eichenberger and Louis-Philippe Morency'], ['Learning to Update Natural Language Comments Based on Code Changes', 'Sheena Panthaplackel, Pengyu Nie, Milos Gligoric, Junyi Jessy Li and Raymond Mooney'], ['Logical Natural Language Generation from Open-Domain Tables', 'Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen and William Yang Wang'], ['Mapping Natural Language Instructions to Mobile UI Action Sequences', 'Yang Li, Jiacong He, Xin Zhou, Yuan Zhang and Jason Baldridge'], ['MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning', 'Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara Berg and Mohit Bansal'], ['Multidirectional Associative Optimization of Function-Specific Word Representations', 'Daniela Gerz, Ivan Vulić, Marek Rei, Roi Reichart and Anna Korhonen'], ['NAT: Noise-Aware Training for Robust Neural Sequence Labeling', 'Marcin Namysl, Sven Behnke and Joachim Köhler'], ['NeuInfer: Knowledge Inference on N-ary Facts', 'Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang and Xueqi Cheng'], ['NILE : Natural Language Inference with Faithful Natural Language Explanations', 'Sawan Kumar and Partha Talukdar'], ['On the Robustness of Language Encoders against Grammatical Errors', 'Fan Yin, Quanyu Long, Tao Meng and Kai-Wei Chang'], ['ParaCrawl: Web-Scale Acquisition of Parallel Corpora', 'Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins and Jaume Zaragoza'], ['Parsing into Variable-in-situ Logico-Semantic Graphs', 'Yufei Chen and Weiwei Sun'], ['Predicting Declension Class from Form and Meaning', 'Adina Williams, Tiago Pimentel, Arya D. McCarthy, Hagen Blix, Eleanor Chodroff and Ryan Cotterell'], ['Predicting Performance for Natural Language Processing Tasks', 'Mengzhou Xia, Antonios Anastasopoulos, Ruochen Xu, Yiming Yang and Graham Neubig'], ['Predicting the Growth of Morphological Families from Social and Linguistic Factors', 'Valentin Hofmann, Janet Pierrehumbert and Hinrich Schütze'], ['Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview', 'Deven Santosh Shah, H. Andrew Schwartz and Dirk Hovy'], ['Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding', 'Sebastian Weigelt, Vanessa Steurer, Tobias Hey and Walter F. Tichy'], ['R^3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge', 'Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan and Nanyun Peng'], ['(Re)construing Meaning in NLP', 'Sean Trott, Tiago Timponi Torrent, Nancy Chang and Nathan Schneider'], ['Roles and Utilization of Attention Heads in Transformer-based Neural Language Models', 'Jae-young Jo and Sung-Hyon Myaeng'], ['S2ORC: The Semantic Scholar Open Research Corpus', 'Kyle Lo, Lucy Wang, Mark Neumann, Rodney Kinney and Daniel Weld'], ['Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora', 'Hila Gonen, Ganesh Jawahar, Djamé Seddah and Yoav Goldberg'], ['Span-based Localizing Network for Natural Language Video Localization', 'Hao Zhang, Aixin Sun, Wei Jing and Joey Tianyi Zhou'], ['Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback', 'Ahmed Elgohary, Saghar Hosseini and Ahmed Hassan Awadallah'], ['Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction', 'Shaowei Chen, Jie Liu, Yu Wang, Wenzheng Zhang and Ziming Chi'], ['Syntax-Aware Opinion Role Labeling with Dependency Graph Convolutional Networks', 'Bo Zhang, Yue Zhang, Rui Wang, Zhenghua Li and Min Zhang'], ['TAG : Type Auxiliary Guiding for Code Comment Generation', 'Ruichu Cai, Zhihao Liang, Boyan Xu, zijian li, Yuexing Hao and Yao Chen'], ['Target Inference in Argument Conclusion Generation', 'Milad Alshomary, Shahbaz Syed, Martin Potthast and Henning Wachsmuth'], ['Translationese as a Language in “Multilingual” NMT', 'Parker Riley, Isaac Caswell, Markus Freitag and David Grangier'], ['Understanding the Language of Political Agreement and Disagreement in Legislative Texts', 'Maryam Davoodi, Eric Waltenburg and Dan Goldwasser'], ['Unsupervised Opinion Summarization as Copycat-Review Generation', 'Arthur Bražinskas, Mirella Lapata and Ivan Titov'], ['USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation', 'Shikib Mehri and Maxine Eskenazi'], ['ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages', 'Colin Lockard, Prashant Shiralkar, Xin Luna Dong and Hannaneh Hajishirzi']], 'base model graph': [['A Graph-based Coarse-to-fine Method for Unsupervised Bilingual Lexicon Induction', 'Shuo Ren, Shujie Liu, Ming Zhou and Shuai Ma'], ['A Joint Neural Model for Information Extraction with Global Features', 'Ying Lin, Heng Ji, Fei Huang and Lingfei Wu'], ['A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks', 'Angela Lin, Sudha Rao, Asli Celikyilmaz, Elnaz Nouri, Chris Brockett, Debadeepta Dey and Bill Dolan'], ['A Span-based Linearization for Constituent Trees', 'Yang Wei, Yuanbin Wu and Man Lan'], ['Adaptive Compression of Word Embeddings', 'Yeachan Kim, Kang-Min Kim and SangKeun Lee'], ['Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis', 'Chunning Du, Haifeng Sun, Jingyu Wang, Qi Qi and Jianxin Liao'], ['An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models', 'Hiroshi Noji and Hiroya Takamura'], ['An Effective Transition-based Model for Discontinuous NER', 'Xiang Dai, Sarvnaz Karimi, Ben Hachey and Cecile Paris'], ['Aspect Sentiment Classification with Document-level Sentiment Preference Modeling', 'Xiao Chen, Changlong Sun, Jingjing Wang, Shoushan Li, Luo Si, Min Zhang and Guodong Zhou'], ['Beyond Possession Existence: Duration and Co-Possession', 'Dhivya Chinnappa, Srikala Murugan and Eduardo Blanco'], ['Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information', 'Michele Bevilacqua and Roberto Navigli'], ['Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction', 'Samuel Broscheit, Kiril Gashteovski, Yanjie Wang and Rainer Gemulla'], ['CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality', 'Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou and Kaicheng Yang'], ['CluHTM - Semantic Hierarchical Topic Modeling based on CluWords', 'Felipe Viegas, Washington Cunha, Christian Gomes, Antônio Pereira, Leonardo Rocha and Marcos Goncalves'], ['Compositionality and Generalization In Emergent Languages', 'Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux and Marco Baroni'], ['Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation', 'Kun Li, Chengbo Chen, Xiaojun Quan, Qing Ling and Yan Song'], ['Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification', 'Hao Tang, Donghong Ji, Chenliang Li and Qiji Zhou'], ['Dialogue-Based Relation Extraction', 'Dian Yu, Kai Sun, Claire Cardie and Dong Yu'], ['Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks', 'Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey and Noah A. Smith'], ['ECPE-2D: Emotion-Cause Pair Extraction based on Joint Two-Dimensional Representation, Interaction and Prediction', 'Zixiang Ding, Rui Xia and Jianfei Yu'], ['Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction', 'Penghui Wei, Jiahao Zhao and Wenji Mao'], ['Emerging Cross-lingual Structure in Pretrained Language Models', 'Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer and Veselin Stoyanov'], ['End-to-End Neural Word Alignment Outperforms GIZA++', 'Thomas Zenkel, Joern Wuebker and John DeNero'], ['ERASER: A Benchmark to Evaluate Rationalized NLP Models', 'Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher and Byron C. Wallace'], ['Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks', 'Fynn Schröder and Chris Biemann'], ['Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with Adversarial Examples', 'Xiaoqing Zheng, Jiehang Zeng, Yi Zhou, Cho-Jui Hsieh, Minhao Cheng and Xuanjing Huang'], ['Feature Projection for Improved Text Classification', 'Qi Qin, Wenpeng Hu and Bing Liu'], ['Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network', 'Yutai Hou, Wanxiang Che, Yongkui Lai, Zhihan Zhou, Yijia Liu, Han Liu and Ting Liu'], ['Fine-grained Interest Matching for Neural News Recommendation', 'Heyuan Wang, Fangzhao Wu, Zheng Liu and Xing Xie'], ['Generalized Entropy Regularization or: There’s Nothing Special about Label Smoothing', 'Clara Meister, Elizabeth Salesky and Ryan Cotterell'], ['Generative Semantic Hashing Enhanced via Boltzmann Machines', 'Lin Zheng, Qinliang Su, Dinghan Shen and Changyou Chen'], ['Heterogeneous Graph Neural Networks for Extractive Document Summarization', 'Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu and Xuanjing Huang'], ['Heterogeneous Graph Transformer for Graph-to-Sequence Learning', 'Shaowei Yao, Tianming Wang and Xiaojun Wan'], ['Hierarchy-Aware Global Model for Hierarchical Text Classification', 'Jie Zhou, Chunping Ma, Dingkun Long, Guangwei Xu, Ning Ding, Haoyu Zhang, Pengjun Xie and Gongshen Liu'], ['Hyperbolic Capsule Networks for Multi-Label Classification', 'Boli Chen, Xin Huang, Lin Xiao and Liping Jing'], ['IMoJIE: Iterative Memory-Based Joint Open Information Extraction', 'Keshav Kolluru, Samarth Aggarwal, Vipul Rathore, Mausam - and Soumen Chakrabarti'], ['Improving Image Captioning Evaluation by Considering Inter References Variance', 'Yanzhi Yi, Hangyu Deng and Jinglu Hu'], ['Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings', 'Apoorv Saxena, Aditay Tripathi and Partha Talukdar'], ['Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings', 'Rishi Bommasani, Kelly Davis and Claire Cardie'], ['Knowledge Graph Embedding Compression', 'Mrinmaya Sachan'], ['Language (technology) is power: The need to be explicit about NLP harms', 'Su Lin Blodgett, Solon Barocas, Hal Daumé III and Hanna Wallach'], ['Learning Architectures from an Extended Search Space for Language Modeling', 'Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu and Changliang Li'], ['Learning Web-based Procedures by Reasoning over Explanations and Demonstrations in Context', 'Shashank Srivastava, Oleksandr Polozov, Nebojsa Jojic and Christopher Meek'], ['Low-Dimensional Hyperbolic Knowledge Graph Embeddings', 'Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi and Christopher Ré'], ['Masked Language Model Scoring', 'Julian Salazar, Davis Liang, Toan Q. Nguyen and Katrin Kirchhoff'], ['Modelling Context and Syntactical Features for Aspect-based Sentiment Analysis', 'Minh Hieu Phan and Philip O. Ogunbona'], ['Neural Syntactic Preordering for Controlled Paraphrase Generation', 'Tanya Goyal and Greg Durrett'], ['Norm-Based Curriculum Learning for Neural Machine Translation', 'Xuebo Liu, Houtim Lai, Derek F. Wong and Lidia S. Chao'], ['Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding', 'Yun Tang, Jing Huang, Guangtao Wang, Xiaodong He and Bowen Zhou'], ['Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models', 'Dan Iter, Kelvin Guu, Larry Lansing and Dan Jurafsky'], ['Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order', 'Yi Liao, Xin Jiang and Qun Liu'], ['Probing for referential information in language models', 'Ionut-Teodor Sorodoc, Kristina Gulordava and Gemma Boleda'], ['Probing Linguistic Features of Sentence-Level Representations in Relation Extraction', 'Christoph Alt, Aleksandra Gabryszak and Leonhard Hennig'], ['ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding', 'Zhiwen Xie, Guangyou Zhou, Jin Liu and Jimmy Xiangji Huang'], ['Relabel the Noise: Joint Extraction of Entities and Relations via Cooperative Multiagents', 'Daoyuan Chen, Yaliang Li, Kai Lei and Ying Shen'], ['Relational Graph Attention Network for Aspect-based Sentiment Analysis', 'Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan and Rui Wang'], ['Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis', 'Zhuang Chen and Tieyun Qian'], ['Review-based Question Generation with Adaptive Instance Transfer and Augmentation', 'Qian Yu, Lidong Bing, Qiong Zhang, Wai Lam and Luo Si'], ['Revisiting the Context Window for Cross-lingual Word Embeddings', 'Ryokan Ri and Yoshimasa Tsuruoka'], ['SEEK: Segmented Embedding of Knowledge Graphs', 'Wentao Xu, Shun Zheng, Liang He, Bin Shao, Jian Yin and Tie-Yan Liu'], ['SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics', 'Da Yin, Tao Meng and Kai-Wei Chang'], ['Similarity Analysis of Contextual Word Representation Models', 'John Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi and James Glass'], ['SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis', 'Hao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He, Hua Wu, Haifeng Wang and Feng Wu'], ['SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization', 'Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao'], ['SpanMlt: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction', 'He Zhao, Longtao Huang, Rong Zhang, Quan Lu and Hui Xue'], ['Suspense in Short Stories is Predicted By Uncertainty Reduction over Neural Story Representation', 'David Wilmot and Frank Keller'], ['TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data', 'Pengcheng Yin, Graham Neubig, Wen-tau Yih and Sebastian Riedel'], ['TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task', 'Christoph Alt, Aleksandra Gabryszak and Leonhard Hennig'], ['Taxonomy Construction of Unseen Domains via Graph-based Cross-Domain Knowledge Transfer', 'Chao Shang, Sarthak Dash, Md Faisal Mahbub Chowdhury, Nandana Mihindukulasooriya and Alfio Gliozzo'], ['The Right Tool for the Job: Matching Model and Instance Complexities', 'Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge and Noah A. Smith'], ['The Sensitivity of Language Models and Humans to Winograd Schema Perturbations', 'Mostafa Abdou, Vinit Ravishankar, Maria Barrett, Yonatan Belinkov, Desmond Elliott and Anders Søgaard'], ['The TechQA Dataset', 'Vittorio Castelli, Rishav Chakravarti, Saswati Dana, Anthony Ferritto, Radu Florian, Martin Franz, Dinesh Garg, Dinesh Khandelwal, Scott McCarley, Michael McCawley, Mohamed Nasr, Lin Pan, Cezar Pendus, John Pitrelli, Saurabh Pujar, Salim Roukos, Andrzej Sakrajda, Avi Sil, Rosario Uceda-Sosa, Todd Ward and Rong Zhang'], ['Transition-based Directed Graph Construction for Emotion-Cause Pair Extraction', 'Chuang Fan, Chaofa Yuan, Jiachen Du, Lin Gui, Min Yang and Ruifeng Xu'], ['Transition-based Semantic Dependency Parsing with Pointer Networks', 'Daniel Fernández-González and Carlos Gómez-Rodríguez'], ['TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition', 'Ruifang He, Jian Wang, Fengyu Guo and Yugui Han'], ['Word-level Textual Adversarial Attacking as Combinatorial Optimization', 'Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu and Maosong Sun'], ['XtremeDistil: Multi-stage Distillation for Massive Multilingual Models', 'Subhabrata Mukherjee and Ahmed Hassan Awadallah']], 'summar model dialogu': [['A Joint Model for Document Segmentation and Segment Labeling', 'Joe Barrow, Rajiv Jain, Vlad Morariu, Varun Manjunatha, Douglas Oard and Philip Resnik'], ['A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation', 'Jan Deriu, Katsiaryna Mlynchyk, Philippe Schläpfer, Alvaro Rodrigo, Dirk von Grünigen, Nicolas Kaiser, Kurt Stockinger, Eneko Agirre and Mark Cieliebak'], ['A Multitask Learning Approach for Diacritic Restoration', 'Sawsan Alqahtani, Ajay Mishra and Mona Diab'], ['A Novel Cascade Binary Tagging Framework for Relational Triple Extraction', 'Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian and Yi Chang'], ['A Prioritization Model for Suicidality Risk Assessment', 'Han-Chin Shing, Philip Resnik and Douglas Oard'], ['A Unified MRC Framework for Named Entity Recognition', 'Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu and Jiwei Li'], ['AdvAug: Robust Adversarial Augmentation for Neural Machine Translation', 'Yong Cheng, Lu Jiang, Wolfgang Macherey and Jacob Eisenstein'], ['Amalgamation of protein sequence, structure and textual information for improving protein-protein interaction identification', 'Pratik Dutta and Sriparna Saha'], ['AMR Parsing with Latent Structural Information', 'Qiji Zhou, Yue Zhang, Donghong Ji and Hao Tang'], ['ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations', 'Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Benoît Sagot and Lucia Specia'], ['Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization', 'Junnan Zhu, Yu Zhou, Jiajun Zhang and Chengqing Zong'], ['Beyond Accuracy: Behavioral Testing of NLP Models with CheckList', 'Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin and Sameer Singh'], ['Bipartite Flat-Graph Network for Nested Named Entity Recognition', 'Ying Luo and Hai Zhao'], ['Code and Named Entity Recognition in StackOverflow', 'Jeniya Tabassum, Mounica Maddela, Wei Xu and Alan Ritter'], ['Coupling Distant Annotation and Adversarial Training for Cross-Domain Chinese Word Segmentation', 'Ning Ding, Dingkun Long, Guangwei Xu, Muhua Zhu, Pengjun Xie, Xiaobin Wang and Haitao Zheng'], ['Discourse as a Function of Event: Profiling Discourse Structure in News Articles around the Main Event', 'Prafulla Kumar Choubey, Aaron Lee, Ruihong Huang and Lu Wang'], ['Discourse-Aware Neural Extractive Text Summarization', 'Jiacheng Xu, Zhe Gan, Yu Cheng and Jingjing Liu'], ['Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction', 'Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova and Katja Markert'], ['Don’t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training', 'Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck, Y-Lan Boureau, Kyunghyun Cho and Jason Weston'], ['Empower Entity Set Expansion via Language Model Probing', 'Yunyi Zhang, Jiaming Shen, Jingbo Shang and Jiawei Han'], ['End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2', 'Donghoon Ham, Jeong-Gwan Lee, Youngsoo Jang and Kee-Eung Kim'], ['Estimating predictive uncertainty for rumour verification models', 'Elena Kochkina and Maria Liakata'], ['Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions', 'Xiaochuang Han, Byron C. Wallace and Yulia Tsvetkov'], ['FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization', 'Esin Durmus, He He and Mona Diab'], ['Finding Universal Grammatical Relations in Multilingual BERT', 'Ethan A. Chi, John Hewitt and Christopher D. Manning'], ['Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer', 'Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini, Kai-Wei Chang and Ahmed Hassan Awadallah'], ['Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation', 'Haoyu Song, Yan Wang, Wei-Nan Zhang, Xiaojiang Liu and Ting Liu'], ['Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs', 'Dong Bok Lee, Seanie Lee, Woo Tae Jeong, Donghwan Kim and Sung Ju Hwang'], ['Graph-to-Tree Learning for Solving Math Word Problems', 'Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao and Ee-Peng Lim'], ['Harvesting and Refining Question-Answer Pairs for Unsupervised QA', 'Zhongli Li, Wenhui Wang, Li Dong, Furu Wei and Ke Xu'], ['History for Visual Dialog: Do we really need it?', 'Shubham Agarwal, Trung Bui, Joon-Young Lee, Ioannis Konstas and Verena Rieser'], ['Hooks in the Headline: Learning to Generate Headlines with Controlled Styles', 'Di Jin, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii and Peter Szolovits'], ['How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems', 'Archiki Prasad and Preethi Jyothi'], ['How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence', 'Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu and Maosong Sun'], ['Improving Event Detection via Open-domain Trigger Knowledge', 'Meihan Tong, Bin Xu, Shuai Wang, Yixin Cao, Lei Hou, Juanzi Li and Jun Xie'], ['Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer', 'Jianfei Yu, Jing Jiang, Li Yang and Rui Xia'], ['Improving Segmentation for Technical Support Problems', 'Kushal Chauhan and Abhirut Gupta'], ['Improving Transformer Models by Reordering their Sublayers', 'Ofir Press, Noah A. Smith and Omer Levy'], ['Information-Theoretic Probing for Linguistic Structure', 'Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams and Ryan Cotterell'], ['Injecting Numerical Reasoning Skills into Language Models', 'Mor Geva, Ankit Gupta and Jonathan Berant'], ['Integrating Multimodal Information in Large Pretrained Transformers', 'Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe Morency and Ehsan Hoque'], ['Interactive Machine Comprehension with Information Seeking Agents', 'Xingdi Yuan, Jie Fu, Marc-Alexandre Côté, Yi Tay, Chris Pal and Adam Trischler'], ['Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?', 'Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann and Samuel R. Bowman'], ['Joint Modelling of Emotion and Abusive Language Detection', 'Santhosh Rajamanickam, Pushkar Mishra, Helen Yannakoudakis and Ekaterina Shutova'], ['Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization', 'Yue Cao, Hui Liu and Xiaojun Wan'], ['KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation', 'Hao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang and Xiaoyan Zhu'], ['Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward', 'Luyang Huang, Lingfei Wu and Lu Wang'], ['Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks', 'Yiping Song, Zequn Liu, Wei Bi, Rui Yan and Ming Zhang'], ['Leveraging Graph to Improve Abstractive Multi-Document Summarization', 'Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang and Junping Du'], ['MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization', 'Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu and Chenliang Li'], ['Meta-Reinforced Multi-Domain State Generator for Dialogue Systems', 'Yi Huang, Junlan Feng, Min Hu, Xiaoting Wu, Xiaoyu Du and Shuo Ma'], ['Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance', 'Prasetya Ajie Utama, Nafise Sadat Moosavi and Iryna Gurevych'], ['MIND: A Large-scale Dataset for News Recommendation', 'Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu and Ming Zhou'], ['More Diverse Dialogue Datasets via Diversity-Informed Data Collection', 'Katherine Stasaski, Grace Hui Yang and Marti A. Hearst'], ['Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering', 'Ming Yan, Hao Zhang, Di Jin and Joey Tianyi Zhou'], ['MuTual: A Dataset for Multi-Turn Dialogue Reasoning', 'Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang and Ming Zhou'], ['Named Entity Recognition without Labelled Data: A Weak Supervision Approach', 'Pierre Lison, Jeremy Barnes, Aliaksandr Hubin and Samia Touileb'], ['Not All Claims are Created Equal: Choosing the Right Statistical Approach to Assess Hypotheses', 'Erfan Sadeqi Azer, Daniel Khashabi, Ashish Sabharwal and Dan Roth'], ['On Faithfulness and Factuality in Abstractive Summarization', 'Joshua Maynez, Shashi Narayan, Bernd Bohnet and Ryan McDonald'], ['Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports', 'Yuhao Zhang, Derek Merck, Emily Tsai, Christopher D. Manning and Curtis Langlotz'], ['Politeness Transfer: A Tag and Generate Approach', 'Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabas Poczos, Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W Black and Shrimai Prabhumoye'], ['Posterior Control of Blackbox Generation', 'Xiang Lisa Li and Alexander Rush'], ['Predicting the Topical Stance and Political Leaning of Media using Tweets', 'Peter Stefanov, Kareem Darwish, Atanas Atanasov and Preslav Nakov'], ['Probing Linguistic Systematicity', 'Emily Goodwin, Koustuv Sinha and Timothy J. O’Donnell'], ['Pyramid: A Layered Model for Nested Named Entity Recognition', 'Jue Wang, Lidan Shou, Ke Chen and Gang Chen'], ['QuASE: Question-Answer Driven Sentence Encoding', 'Hangfeng He, Qiang Ning and Dan Roth'], ['Reasoning with Latent Structure Refinement for Document-Level Relation Extraction', 'Guoshun Nan, Zhijiang Guo, Ivan Sekulic and Wei Lu'], ['Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network', 'Yangming Li, Kaisheng Yao, Libo Qin, Wanxiang Che, Xiaolong Li and Ting Liu'], ['Sources of Transfer in Multilingual Named Entity Recognition', 'David Mueller, Nicholas Andrews and Mark Dredze'], ['Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words', 'Josef Klafka and Allyson Ettinger'], ['Structural Information Preserving for Graph-to-Text Generation', 'Linfeng Song, Ante Wang, Jinsong Su, Yue Zhang, Kun Xu, Yubin Ge and Dong Yu'], ['Temporal Common Sense Acquisition with Minimal Supervision', 'Ben Zhou, Qiang Ning, Daniel Khashabi and Dan Roth'], ['Temporally-Informed Analysis of Named Entity Recognition', 'Shruti Rijhwani and Daniel Preotiuc-Pietro'], ['“The Boating Store Had Its Best Sail Ever”: Pronunciation-attentive Contextualized Pun Recognition', 'Yichao Zhou, Jyun-Yu Jiang, Jieyu Zhao, Kai-Wei Chang and Wei Wang'], ['The Unstoppable Rise of Computational Linguistics in Deep Learning', 'James Henderson'], ['Towards Robustifying NLI Models Against Lexical Dataset Biases', 'Xiang Zhou and Mohit Bansal'], ['Unsupervised Domain Clusters in Pretrained Language Models', 'Roee Aharoni and Yoav Goldberg'], ['Unsupervised Opinion Summarization with Noising and Denoising', 'Reinald Kim Amplayo and Mirella Lapata'], ['Unsupervised Paraphrasing by Simulated Annealing', 'Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou and Sen Song'], ['Weight Poisoning Attacks on Pretrained Models', 'Keita Kurita, Paul Michel and Graham Neubig'], ['What Question Answering can Learn from Trivia Nerds', 'Jordan Boyd-Graber and Benjamin Börschinger']]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|▏         | 1/71 [00:02<02:58,  2.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 2/71 [00:04<02:50,  2.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 3/71 [00:07<02:44,  2.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 4/71 [00:09<02:42,  2.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 5/71 [00:12<02:40,  2.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 6/71 [00:14<02:40,  2.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|▉         | 7/71 [00:17<02:43,  2.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 11%|█▏        | 8/71 [00:19<02:38,  2.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 13%|█▎        | 9/71 [00:22<02:34,  2.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 10/71 [00:24<02:27,  2.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▌        | 11/71 [00:26<02:22,  2.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 17%|█▋        | 12/71 [00:28<02:17,  2.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 13/71 [00:31<02:13,  2.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 20%|█▉        | 14/71 [00:33<02:16,  2.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 21%|██        | 15/71 [00:36<02:14,  2.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "distilling annotations via active imitation learning\n",
            "random expert distillation: imitation learning via expert policy support estimation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 23%|██▎       | 16/71 [00:38<02:11,  2.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▍       | 17/71 [00:40<02:09,  2.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 25%|██▌       | 18/71 [00:43<02:07,  2.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 27%|██▋       | 19/71 [00:45<02:04,  2.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 20/71 [00:48<02:01,  2.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 30%|██▉       | 21/71 [00:50<01:58,  2.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 31%|███       | 22/71 [00:52<01:57,  2.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 23/71 [00:55<01:59,  2.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▍      | 24/71 [00:57<01:52,  2.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 35%|███▌      | 25/71 [01:00<01:50,  2.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 37%|███▋      | 26/71 [01:02<01:50,  2.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "graph neural news recommendation with unsupervised preference disentanglement\n",
            "graph neural news recommendation with long-term and short-term interest modeling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 27/71 [01:05<01:47,  2.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 39%|███▉      | 28/71 [01:07<01:45,  2.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 41%|████      | 29/71 [01:10<01:41,  2.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 42%|████▏     | 30/71 [01:12<01:36,  2.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 44%|████▎     | 31/71 [01:14<01:34,  2.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 45%|████▌     | 32/71 [01:17<01:32,  2.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▋     | 33/71 [01:19<01:27,  2.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 48%|████▊     | 34/71 [01:21<01:26,  2.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 49%|████▉     | 35/71 [01:23<01:21,  2.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 51%|█████     | 36/71 [01:26<01:20,  2.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 37/71 [01:28<01:17,  2.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 54%|█████▎    | 38/71 [01:30<01:14,  2.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 55%|█████▍    | 39/71 [01:32<01:13,  2.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▋    | 40/71 [01:35<01:10,  2.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 58%|█████▊    | 41/71 [01:37<01:09,  2.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 59%|█████▉    | 42/71 [01:40<01:08,  2.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 61%|██████    | 43/71 [01:42<01:04,  2.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▏   | 44/71 [01:44<01:03,  2.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 63%|██████▎   | 45/71 [01:46<00:58,  2.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 65%|██████▍   | 46/71 [01:49<00:58,  2.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 66%|██████▌   | 47/71 [01:51<00:56,  2.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 68%|██████▊   | 48/71 [01:53<00:52,  2.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 69%|██████▉   | 49/71 [01:56<00:52,  2.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 70%|███████   | 50/71 [01:58<00:49,  2.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 72%|███████▏  | 51/71 [02:01<00:48,  2.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 73%|███████▎  | 52/71 [02:03<00:46,  2.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 75%|███████▍  | 53/71 [02:06<00:43,  2.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 76%|███████▌  | 54/71 [02:08<00:40,  2.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 77%|███████▋  | 55/71 [02:10<00:38,  2.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 79%|███████▉  | 56/71 [02:13<00:35,  2.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 80%|████████  | 57/71 [02:15<00:32,  2.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 82%|████████▏ | 58/71 [02:17<00:29,  2.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 83%|████████▎ | 59/71 [02:19<00:27,  2.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 85%|████████▍ | 60/71 [02:22<00:25,  2.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 86%|████████▌ | 61/71 [02:24<00:23,  2.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 87%|████████▋ | 62/71 [02:27<00:21,  2.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 89%|████████▊ | 63/71 [02:29<00:19,  2.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 90%|█████████ | 64/71 [02:32<00:17,  2.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 92%|█████████▏| 65/71 [02:34<00:14,  2.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 93%|█████████▎| 66/71 [02:36<00:12,  2.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 94%|█████████▍| 67/71 [02:39<00:09,  2.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 96%|█████████▌| 68/71 [02:41<00:07,  2.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 97%|█████████▋| 69/71 [02:44<00:04,  2.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 99%|█████████▊| 70/71 [02:46<00:02,  2.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "unknown intent detection using gaussian mixture model with an application to zero-shot intent classification\n",
            "zero-shot user intent detection via capsule neural networks\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 71/71 [02:49<00:00,  2.39s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Gk2bFBdqCNRI",
        "colab_type": "code",
        "outputId": "bdbd8eec-b5de-4ad3-f6bc-ae612f11255b",
        "colab": {}
      },
      "source": [
        "with open(\"papers_with_arxiv_link.md\", \"w\") as f:\n",
        "    f.write(\"## Long Papers\\n\\n\")\n",
        "    generate_paper_list_with_arxiv_link(f, longp)\n",
        "    f.write(\"## Short Papers\\n\\n\")\n",
        "    generate_paper_list_with_arxiv_link(f, short)\n",
        "    f.write(\"## System Demonstrations\\n\\n\")\n",
        "    generate_paper_list_with_arxiv_link(f, demo)\n",
        "    f.write(\"## Student Research Workshop\\n\\n\")\n",
        "    generate_paper_list_with_arxiv_link(f, student)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  5%|▌         | 31/571 [01:07<21:59,  2.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "adaptive compression of word embeddings\n",
            "online embedding compression for text classification using low rank matrix factorization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 32/571 [01:10<22:15,  2.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "addressing posterior collapse with mutual information for improved variational neural machine translation\n",
            "improved variational neural machine translation by promoting mutual information\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 53/571 [01:54<18:55,  2.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "attentive pooling with learnable norms for text representation\n",
            "attentive pooling networks\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 68/571 [02:31<21:03,  2.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "bilingual dictionary based neural machine translation without using parallel sentences\n",
            "bridging neural machine translation and bilingual dictionaries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 73/571 [02:43<19:23,  2.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "boosting neural machine translation with similar translations\n",
            "neural machine translation from simplified translations\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 96/571 [03:36<18:33,  2.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "contextualized weak supervision for text classification\n",
            "weakly-supervised neural text classification\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 103/571 [03:50<17:57,  2.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "cross-lingual unsupervised sentiment classification with multi-view transfer learning\n",
            "multi-source cross-lingual model transfer: learning what to share\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 109/571 [04:07<19:52,  2.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "curriculum learning for natural language understanding\n",
            "visualizing and understanding curriculum learning for long short-term memory networks\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 126/571 [04:50<17:29,  2.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "distilling annotations via active imitation learning\n",
            "random expert distillation: imitation learning via expert policy support estimation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 147/571 [05:45<17:43,  2.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "effective inter-clause modeling for end-to-end emotion-cause pair extraction\n",
            "end-to-end emotion-cause pair extraction via learning to link\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 31%|███       | 176/571 [06:53<17:02,  2.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "explicit semantic decomposition for definition generation\n",
            "semantic composition and decomposition: from recognition to generation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 218/571 [08:31<14:40,  2.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "graph neural news recommendation with unsupervised preference disentanglement\n",
            "graph neural news recommendation with long-term and short-term interest modeling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 251/571 [09:43<11:39,  2.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "improving disentangled text representation learning with information-theoretic guidance\n",
            "improving disentangled representation learning with the beta bernoulli process\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 255/571 [09:52<11:58,  2.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "improving image captioning with better use of caption\n",
            "hidden state guidance: improving image captioning using an image conditioned autoencoder\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 264/571 [10:15<12:37,  2.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "in neural machine translation, what does transfer learning transfer?\n",
            "exploring benefits of transfer learning in neural machine translation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 303/571 [11:47<10:44,  2.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "learning constraints for structured prediction using rectifier networks\n",
            "adversarial constraint learning for structured prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 308/571 [11:58<10:49,  2.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "learning to ask more: semi-autoregressive sequential question generation under dual-graph interaction\n",
            "semi-autoregressive neural machine translation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 325/571 [12:39<10:59,  2.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "low-resource generation of multi-hop reasoning questions\n",
            "reinforced multi-task approach for multi-hop question generation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 333/571 [12:57<09:05,  2.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "meta-reinforced multi-domain state generator for dialogue systems\n",
            "transferable multi-domain state generator for task-oriented dialogue systems\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 354/571 [13:43<08:24,  2.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "multi-hypothesis machine translation evaluation\n",
            "pairwise neural machine translation evaluation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 71%|███████▏  | 407/571 [15:50<05:31,  2.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "predicting the topical stance and political leaning of media using tweets\n",
            "predicting the topical stance of media and popular twitter users\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 409/571 [15:55<06:08,  2.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "premise selection in natural language mathematical texts\n",
            "natural language premise selection: finding supporting statements for mathematical text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 434/571 [16:59<05:52,  2.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "reinceptione: relation-aware inception network with joint local-global structural information for knowledge graph embedding\n",
            "relation-aware entity alignment for heterogeneous knowledge graphs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 492/571 [19:24<03:03,  2.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "structural information preserving for graph-to-text generation\n",
            "structural neural encoders for amr-to-text generation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 547/571 [21:33<00:55,  2.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "unknown intent detection using gaussian mixture model with an application to zero-shot intent classification\n",
            "zero-shot user intent detection via capsule neural networks\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 569/571 [22:29<00:04,  2.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "zero-shot text classification via reinforced self-training\n",
            "transductive zero-shot learning with a self-training dictionary approach\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 571/571 [22:33<00:00,  2.35s/it]\n",
            " 13%|█▎        | 28/208 [01:03<06:52,  2.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "camouflaged chinese spam content detection with semi-supervised generative active learning\n",
            "gans for semi-supervised opinion spam detection\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 37/208 [01:26<07:13,  2.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "content word aware neural machine translation\n",
            "selective attention for context-aware neural machine translation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 66/208 [02:53<10:01,  4.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "entity-aware dependency-based deep graph attention network for comparative preference classification\n",
            "exploiting typed syntactic dependencies for targeted sentiment classification using graph attention neural network\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 93/208 [04:07<05:58,  3.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "interpretable operational risk classification with semi-supervised variational autoencoder\n",
            "disentangled variational auto-encoder for semi-supervised learning\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 102/208 [04:29<04:46,  2.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "learning low-resource end-to-end goal-oriented dialog for fast and reliable system deployment\n",
            "learning end-to-end goal-oriented dialog\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 121/208 [05:21<05:06,  3.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "multimodal and multiresolution speech recognition with transformers\n",
            "multiresolution and multimodal speech recognition with transformers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 61%|██████    | 126/208 [05:33<03:56,  2.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "neural graph matching networks for chinese short text matching\n",
            "graph matching networks for learning the similarity of graph structured objects\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 189/208 [08:17<00:52,  2.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "tree-structured neural topic model\n",
            "structured neural topic models for reviews\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 207/208 [09:01<00:02,  2.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "``you sound just like your father’’ commercial machine translation systems include stylistic biases\n",
            "reducing gender bias in neural machine translation as a domain adaptation problem\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 208/208 [09:03<00:00,  2.49s/it]\n",
            "100%|██████████| 43/43 [01:42<00:00,  2.15s/it]\n",
            "  4%|▍         | 2/49 [00:04<01:36,  2.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "a geometry-inspired attack for generating natural language adversarial examples\n",
            "a geometry-inspired decision-based attack\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 24/49 [00:50<00:53,  2.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "υbleu: uncertainty-aware automatic evaluation method for open-domain dialogue systems\n",
            "better automatic evaluation of open-domain dialogue systems with contextualized embeddings\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 49/49 [01:43<00:00,  1.84s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKaDBI55CNRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}