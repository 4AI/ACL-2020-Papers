{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "generate_paper_list_with_arxiv_link.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1m2RpTeiCNQq"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hunkim/ACL-2020-Papers/blob/master/generate_paper_list_with_arxiv_link.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m2RpTeiCNQq",
        "colab_type": "text"
      },
      "source": [
        "# Load Paper List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G-XWU8JCNQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_papers(path):\n",
        "    papers = [[]]\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                papers[-1].append(line)\n",
        "            else:\n",
        "                papers.append([])\n",
        "    for p in papers:\n",
        "        assert len(p) == 2\n",
        "    return papers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_5mqAPzCNQu",
        "colab_type": "code",
        "outputId": "c0f334d7-f482-4fdd-85d9-c00a3d7620ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "longp = read_papers(\"./data/long.txt\")\n",
        "longp[:3]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['2kenize: Tying Subword Sequences for Chinese Script Conversion',\n",
              "  'Pranav A and Isabelle Augenstein'],\n",
              " ['A Batch Normalized Inference Network Keeps the KL Vanishing Away',\n",
              "  'Qile Zhu, Wei Bi, Xiaojiang Liu, Xiyao Ma, Xiaolin Li and Dapeng Wu'],\n",
              " ['A Call for More Rigor in Unsupervised Cross-lingual Learning',\n",
              "  'Mikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka and Eneko Agirre']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxPZti17CNQx",
        "colab_type": "code",
        "outputId": "25cf494d-474b-467a-852b-16552b768dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(longp)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqL_0bNPCNQ0",
        "colab_type": "code",
        "outputId": "5ec4c616-6f2f-4d7f-d1ac-214e2848687f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "short = read_papers(\"./data/short.txt\")\n",
        "short[:3]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle',\n",
              "  'Shyh-Shiun Hung, Hen-Hsen Huang and Hsin-Hsi Chen'],\n",
              " ['A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers',\n",
              "  'Shen-yun Miao, Chao-Chun Liang and Keh-Yih Su'],\n",
              " ['A Frame-based Sentence Representation for Machine Reading Comprehension',\n",
              "  'Shaoru Guo, Ru Li, Hongye Tan, Xiaoli Li, Yong Guan, Hongyan Zhao and Yueping Zhang']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLNXH4MJCNQ2",
        "colab_type": "code",
        "outputId": "e63030a0-7c2c-47b4-b5bc-11ca29380a25",
        "colab": {}
      },
      "source": [
        "len(short)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrBRORv3CNQ5",
        "colab_type": "code",
        "outputId": "715cc350-82da-4eb2-fd70-986ad051fb0c",
        "colab": {}
      },
      "source": [
        "demo = read_papers(\"./data/demo.txt\")\n",
        "demo[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents',\n",
              "  'Chia-Yu Li, Daniel Ortega, Dirk Väth, Florian Lux, Lindsey Vanderlyn, Maximilian Schmidt, Michael Neumann, Moritz Völkel, Pavel Denisov, Sabrina Jenne, Zorica Kacarevic and Ngoc Thang Vu'],\n",
              " ['BENTO: A Visual Platform for Building Clinical NLP Pipelines Based on CodaLab',\n",
              "  'Yonghao Jin, Fei Li and Hong Yu'],\n",
              " ['Clinical-Coder: Assigning Interpretable ICD-10 Codes to Chinese Clinical Notes',\n",
              "  'Pengfei Cao, Chenwei Yan, xiangling fu, Yubo Chen, Kang Liu, Jun Zhao, Shengping Liu and Weifeng Chong']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GySLkygHCNQ7",
        "colab_type": "code",
        "outputId": "bc0031db-864e-4a43-98d6-4f3c7fa23d47",
        "colab": {}
      },
      "source": [
        "len(demo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cNcBqCsCNQ-",
        "colab_type": "code",
        "outputId": "4d233334-f54a-47aa-e788-dcfec4eabeaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "student = read_papers(\"./data/student.txt\")\n",
        "student[:3]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['#NotAWhore! A Computational Linguistic Perspective of Rape Culture and Victimization on Social Media',\n",
              "  'Ashima Suvarna and Grusha Bhalla'],\n",
              " ['A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples',\n",
              "  'Zhao Meng and Roger Wattenhofer'],\n",
              " ['A Simple and Effective Dependency parser for Telugu',\n",
              "  'Sneha Nallani, Manish Shrivastava and Dipti Sharma']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Zc2x0qCNRA",
        "colab_type": "code",
        "outputId": "49525fec-44d2-4130-8fce-077758220a61",
        "colab": {}
      },
      "source": [
        "len(student)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw4p7l8eCYX2",
        "colab_type": "text"
      },
      "source": [
        "# Sorting by Topic\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c36Fa3LPCovq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fd83c698-ed03-4b8a-f544-86bd403549eb"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "  return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "  result=[]\n",
        "  for token in gensim.utils.simple_preprocess(text) :\n",
        "    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "      result.append(lemmatize_stemming(token))\n",
        "                \n",
        "  return result\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqDjrwyoHEPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FIXME: Better way to get human readable topic names from LDA topics?\n",
        "def list2topiclist(list, num_topics = 8):\n",
        "  processed_docs = []\n",
        "  for line in list:\n",
        "    processed_line = preprocess(line[0])\n",
        "    processed_docs.append(processed_line)\n",
        "\n",
        "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "  \n",
        "  lda = gensim.models.LdaModel(bow_corpus, num_topics, \n",
        "                               id2word = dictionary, passes = 10)\n",
        "\n",
        "\n",
        "  def get_topic_title(idx, topn=3):\n",
        "    topn_terms = [dictionary[x[0]] for x in lda.get_topic_terms(idx, topn)]\n",
        "    return \" \".join(topn_terms)\n",
        "\n",
        "  # Create topic title\n",
        "  list_topic_titles = []\n",
        "  for i in range(num_topics):\n",
        "    list_topic_titles.append(get_topic_title(i))\n",
        "\n",
        "  # Assign list to topic\n",
        "  topic_dict = {}\n",
        "  for line in list:\n",
        "    processed_line = preprocess(line[0])\n",
        "    bow_vector = dictionary.doc2bow(processed_line)\n",
        "    line_topic = sorted(lda.get_document_topics(bow_vector), \n",
        "                        key=lambda tup: tup[1], reverse=True)\n",
        "    topic_title = list_topic_titles[line_topic[0][0]]\n",
        "\n",
        "    if topic_title not in topic_dict:\n",
        "      topic_dict[topic_title] = []\n",
        "\n",
        "    topic_dict[topic_title].append(line)\n",
        "  \n",
        "  return topic_dict\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw8jDBlNNGYH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "de32f3f8-f714-4258-b346-0fb20ba402f9"
      },
      "source": [
        "topic_long = list2topiclist(longp)\n",
        "for topic in topic_long:\n",
        "  print(topic)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['model languag dialogu', 'model languag generat', 'languag relat natur', 'machin translat learn', 'cross neural pars', 'generat semant question', 'generat learn model', 'languag word extract']\n",
            "languag relat natur\n",
            "generat semant question\n",
            "cross neural pars\n",
            "languag word extract\n",
            "generat learn model\n",
            "model languag generat\n",
            "model languag dialogu\n",
            "machin translat learn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUjlRSPtCNRD",
        "colab_type": "text"
      },
      "source": [
        "# Search arXiv Link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws04-fbRCNRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googlesearch import search\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "from difflib import SequenceMatcher\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def similarity(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "\n",
        "def search_arxiv_link(title, sleep=1):\n",
        "    time.sleep(sleep)\n",
        "    link = None\n",
        "    for j in search(title, tld=\"co.in\", num=10, stop=1, pause=0.5):\n",
        "        if 'arxiv.org/abs' in j:\n",
        "            thepage = urllib.request.urlopen(j)\n",
        "            soup = BeautifulSoup(thepage, \"html.parser\")\n",
        "            searched_title = ' '.join(soup.title.text.lower().split()[1:])\n",
        "            if similarity(title, searched_title) > 0.8:\n",
        "                link = j\n",
        "                break\n",
        "            else:\n",
        "                print(\"NOT MATCHED\")\n",
        "                print(title)\n",
        "                print(searched_title)\n",
        "    return link"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAJu9189CNRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_paper_list_with_arxiv_link(f, papers):\n",
        "    for p in tqdm(papers):\n",
        "        title, authors = p\n",
        "        link = search_arxiv_link(title.lower())\n",
        "        if link:\n",
        "            f.write(f\"- {title} [[arXiv]]({link})\\n\")\n",
        "        else:\n",
        "            f.write(f\"- {title}\\n\")\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "796iR6xmLnMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_paper_list_with_arxiv_link_topic(f, papers):\n",
        "  topic_papers = list2topiclist(papers)\n",
        "  for topic in topic_papers:\n",
        "    f.write(\"###\" + topic + \"\\n\")\n",
        "    generate_paper_list_with_arxiv_link(f, topic_papers[topic])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11VvQQBRMZjY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "44c240d4-1dd5-4653-ddcb-d56d6b2a4e70"
      },
      "source": [
        "with open(\"papers_with_arxiv_link_topic.md\", \"w\") as f:\n",
        "  f.write(\"## Long Papers\\n\\n\")\n",
        "  generate_paper_list_with_arxiv_link_topic(f, longp)\n",
        "  f.write(\"## Short Papers\\n\\n\")\n",
        "  generate_paper_list_with_arxiv_link_topic(f, short)\n",
        "  f.write(\"## System Demonstrations\\n\\n\")\n",
        "  generate_paper_list_with_arxiv_link_topic(f, demo)\n",
        "  f.write(\"## Student Research Workshop\\n\\n\")\n",
        "  generate_paper_list_with_arxiv_link_topic(f, student)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['languag model natur', 'graph generat inform', 'learn multi detect', 'semant learn model', 'cross model lingual', 'word generat domain', 'languag natur understand', 'neural translat machin']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/87 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 1/87 [00:02<03:32,  2.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 2/87 [00:05<03:36,  2.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 3/87 [00:07<03:33,  2.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▍         | 4/87 [00:09<03:21,  2.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 5/87 [00:12<03:11,  2.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 6/87 [00:14<03:09,  2.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 7/87 [00:16<03:10,  2.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 8/87 [00:18<03:02,  2.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Gk2bFBdqCNRI",
        "colab_type": "code",
        "outputId": "bdbd8eec-b5de-4ad3-f6bc-ae612f11255b",
        "colab": {}
      },
      "source": [
        "with open(\"papers_with_arxiv_link.md\", \"w\") as f:\n",
        "    f.write(\"## Long Papers\\n\\n\")\n",
        "    generate_paper_list_with_arxiv_link(f, longp)\n",
        "    f.write(\"## Short Papers\\n\\n\")\n",
        "    generate_paper_list_with_arxiv_link(f, short)\n",
        "    f.write(\"## System Demonstrations\\n\\n\")\n",
        "    generate_paper_list_with_arxiv_link(f, demo)\n",
        "    f.write(\"## Student Research Workshop\\n\\n\")\n",
        "    generate_paper_list_with_arxiv_link(f, student)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  5%|▌         | 31/571 [01:07<21:59,  2.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "adaptive compression of word embeddings\n",
            "online embedding compression for text classification using low rank matrix factorization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 32/571 [01:10<22:15,  2.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "addressing posterior collapse with mutual information for improved variational neural machine translation\n",
            "improved variational neural machine translation by promoting mutual information\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 53/571 [01:54<18:55,  2.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "attentive pooling with learnable norms for text representation\n",
            "attentive pooling networks\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 68/571 [02:31<21:03,  2.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "bilingual dictionary based neural machine translation without using parallel sentences\n",
            "bridging neural machine translation and bilingual dictionaries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 73/571 [02:43<19:23,  2.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "boosting neural machine translation with similar translations\n",
            "neural machine translation from simplified translations\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 96/571 [03:36<18:33,  2.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "contextualized weak supervision for text classification\n",
            "weakly-supervised neural text classification\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 103/571 [03:50<17:57,  2.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "cross-lingual unsupervised sentiment classification with multi-view transfer learning\n",
            "multi-source cross-lingual model transfer: learning what to share\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 109/571 [04:07<19:52,  2.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "curriculum learning for natural language understanding\n",
            "visualizing and understanding curriculum learning for long short-term memory networks\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 126/571 [04:50<17:29,  2.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "distilling annotations via active imitation learning\n",
            "random expert distillation: imitation learning via expert policy support estimation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 147/571 [05:45<17:43,  2.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "effective inter-clause modeling for end-to-end emotion-cause pair extraction\n",
            "end-to-end emotion-cause pair extraction via learning to link\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 31%|███       | 176/571 [06:53<17:02,  2.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "explicit semantic decomposition for definition generation\n",
            "semantic composition and decomposition: from recognition to generation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 218/571 [08:31<14:40,  2.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "graph neural news recommendation with unsupervised preference disentanglement\n",
            "graph neural news recommendation with long-term and short-term interest modeling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 251/571 [09:43<11:39,  2.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "improving disentangled text representation learning with information-theoretic guidance\n",
            "improving disentangled representation learning with the beta bernoulli process\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 255/571 [09:52<11:58,  2.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "improving image captioning with better use of caption\n",
            "hidden state guidance: improving image captioning using an image conditioned autoencoder\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 264/571 [10:15<12:37,  2.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "in neural machine translation, what does transfer learning transfer?\n",
            "exploring benefits of transfer learning in neural machine translation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 303/571 [11:47<10:44,  2.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "learning constraints for structured prediction using rectifier networks\n",
            "adversarial constraint learning for structured prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 308/571 [11:58<10:49,  2.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "learning to ask more: semi-autoregressive sequential question generation under dual-graph interaction\n",
            "semi-autoregressive neural machine translation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 325/571 [12:39<10:59,  2.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "low-resource generation of multi-hop reasoning questions\n",
            "reinforced multi-task approach for multi-hop question generation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 333/571 [12:57<09:05,  2.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "meta-reinforced multi-domain state generator for dialogue systems\n",
            "transferable multi-domain state generator for task-oriented dialogue systems\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 354/571 [13:43<08:24,  2.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "multi-hypothesis machine translation evaluation\n",
            "pairwise neural machine translation evaluation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 71%|███████▏  | 407/571 [15:50<05:31,  2.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "predicting the topical stance and political leaning of media using tweets\n",
            "predicting the topical stance of media and popular twitter users\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 409/571 [15:55<06:08,  2.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "premise selection in natural language mathematical texts\n",
            "natural language premise selection: finding supporting statements for mathematical text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 434/571 [16:59<05:52,  2.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "reinceptione: relation-aware inception network with joint local-global structural information for knowledge graph embedding\n",
            "relation-aware entity alignment for heterogeneous knowledge graphs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 492/571 [19:24<03:03,  2.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "structural information preserving for graph-to-text generation\n",
            "structural neural encoders for amr-to-text generation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 547/571 [21:33<00:55,  2.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "unknown intent detection using gaussian mixture model with an application to zero-shot intent classification\n",
            "zero-shot user intent detection via capsule neural networks\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 569/571 [22:29<00:04,  2.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "zero-shot text classification via reinforced self-training\n",
            "transductive zero-shot learning with a self-training dictionary approach\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 571/571 [22:33<00:00,  2.35s/it]\n",
            " 13%|█▎        | 28/208 [01:03<06:52,  2.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "camouflaged chinese spam content detection with semi-supervised generative active learning\n",
            "gans for semi-supervised opinion spam detection\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 37/208 [01:26<07:13,  2.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "content word aware neural machine translation\n",
            "selective attention for context-aware neural machine translation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 66/208 [02:53<10:01,  4.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "entity-aware dependency-based deep graph attention network for comparative preference classification\n",
            "exploiting typed syntactic dependencies for targeted sentiment classification using graph attention neural network\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 93/208 [04:07<05:58,  3.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "interpretable operational risk classification with semi-supervised variational autoencoder\n",
            "disentangled variational auto-encoder for semi-supervised learning\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 102/208 [04:29<04:46,  2.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "learning low-resource end-to-end goal-oriented dialog for fast and reliable system deployment\n",
            "learning end-to-end goal-oriented dialog\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 121/208 [05:21<05:06,  3.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "multimodal and multiresolution speech recognition with transformers\n",
            "multiresolution and multimodal speech recognition with transformers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 61%|██████    | 126/208 [05:33<03:56,  2.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "neural graph matching networks for chinese short text matching\n",
            "graph matching networks for learning the similarity of graph structured objects\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 189/208 [08:17<00:52,  2.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "tree-structured neural topic model\n",
            "structured neural topic models for reviews\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 207/208 [09:01<00:02,  2.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "``you sound just like your father’’ commercial machine translation systems include stylistic biases\n",
            "reducing gender bias in neural machine translation as a domain adaptation problem\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 208/208 [09:03<00:00,  2.49s/it]\n",
            "100%|██████████| 43/43 [01:42<00:00,  2.15s/it]\n",
            "  4%|▍         | 2/49 [00:04<01:36,  2.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "a geometry-inspired attack for generating natural language adversarial examples\n",
            "a geometry-inspired decision-based attack\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 24/49 [00:50<00:53,  2.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NOT MATCHED\n",
            "υbleu: uncertainty-aware automatic evaluation method for open-domain dialogue systems\n",
            "better automatic evaluation of open-domain dialogue systems with contextualized embeddings\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 49/49 [01:43<00:00,  1.84s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKaDBI55CNRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}